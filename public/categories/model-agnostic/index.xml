<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>model-agnostic on Ethical AI Tool Landscape</title>
    <link>http://example.org/categories/model-agnostic/</link>
    <description>Recent content in model-agnostic on Ethical AI Tool Landscape</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Edwin Wenink</copyright>
    
	<atom:link href="http://example.org/categories/model-agnostic/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Aequitas: Bias and Fairness Audit Toolkit</title>
      <link>http://example.org/tools/aequitas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/aequitas/</guid>
      <description>Audit The Aequitas toolkit can both be used on the command-line, programmatically via its Python API or via a web interface. The web interface offers a four step programme to audit a dataset on bias. The four steps are:
 Upload (tabular) data Determine protected groups and reference group Select fairness metrics and disparity intolerance Inspect bias report  Example audit report.    This toolkit is useful for auditing bias and fairness according to a limited set of common fairness metrics, but does not offer algorithms for mitigating bias.</description>
    </item>
    
    <item>
      <title>Agile Ethics for AI</title>
      <link>http://example.org/tools/agile_ethics_for_ai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/agile_ethics_for_ai/</guid>
      <description>Butnaru and others associated with the HAI center at Stanford set up a Agile Ethics workflow in the form of a Trello board. From left to right, the workflow walks you through relevant ethical considerations at the various steps of a machine learning pipeline. The phases are:
 Scope  Consider ethical implications of the project Consider skill mapping (what&amp;rsquo;s the impact of AI on jobs)?  Facilitates up-skilling or a change of strategy in the use of human talent     Data audit  Led by Chief Data Officer &amp;ldquo;Meet and plan&amp;rdquo; stage in Agile Helpful: Data Ethics Canvas   Train  Build stage in Agile Consider (tools for) transparency and fairness   Analyse  Benchmarks, including benchmarks related to e.</description>
    </item>
    
    <item>
      <title>AI Ethics Guidelines Global Inventory</title>
      <link>http://example.org/tools/algorithmwatch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/algorithmwatch/</guid>
      <description>AlgorithmWatch is maintaining a searchable inventory of published frameworks that set out ethical AI values. They can be searched on sector/actor, type, region and location.
AlgorithmWatch noted some common patterns here after publishing the first version of the index:
 &amp;ldquo;All include the similar principles on transparency, equality/non-discrimination, accountability and safety. Some add additional principles, such as the demand for AI be socially beneficial and protect human rights.&amp;rdquo; &amp;ldquo;Most frameworks are developed by coalitions, or institutions such as universities that then invite companies and individuals to sign up to these.</description>
    </item>
    
    <item>
      <title>Algorithmic Accountability Policy Toolkit</title>
      <link>http://example.org/tools/algorithmic-accountability-policy-toolkit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/algorithmic-accountability-policy-toolkit/</guid>
      <description>AI Now published the Algorithmic Accountability Policy Toolkit in 2018. It is specifically tailored towards advocates concerned with government use of algorithms. The toolkit provides a FAQ, an overview of various types of algorithms used by governments in specific application areas such as public health or criminal justice, and a comprehensive list of relevant literature.
The fact that this is a toolkit and not a paper can be seen from the very practical guidance that is offered.</description>
    </item>
    
    <item>
      <title>ART: Adversial Robustness 360 Toolbox</title>
      <link>http://example.org/tools/art/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/art/</guid>
      <description>The Adversial Robustness Toolbox (ART) is the first comprehensive toolbox that unifies many defensive techniques for four categories of adversial attacks on machine learning models. These categories are model evasion, model poisoning, model extraction and inference (e.g. inference of sensitive attributes in the training data; or determining whether an example was part of the training data). ART supports all popular machine learning frameworks, all data types and all machine learning tasks.</description>
    </item>
    
    <item>
      <title>Data Ethics Canvas</title>
      <link>http://example.org/tools/data-ethics-canvas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/data-ethics-canvas/</guid>
      <description>The Data Ethics Canvas is a tool developed by the Open Data Institute for providing ethical guidance to organizations doing any type of project involving data. That includes data collection, sharing, and its usage for example in machine learning applications. The tool is accompanied with a white paper and a brief practical guide for its usage.
Page 3 of the practical guide lists some recommendations that are also relevant when you do not use this tool.</description>
    </item>
    
    <item>
      <title>Datasheets for Datasets</title>
      <link>http://example.org/tools/datasheet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/datasheet/</guid>
      <description>The method described in this paper aids in documenting datasets to help avoid unwanted consequences of data usage.
Abstract:
 The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information.</description>
    </item>
    
    <item>
      <title>DEDA: De Ethische Data Assistent</title>
      <link>http://example.org/tools/deda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/deda/</guid>
      <description>This toolkit developed by the Utrecht Data School supports data analysts, projectmanagers, and policy makers in identifying ethical values and issues in data projects and promoting accountability towards stakeholders. The toolkit is written in Dutch and includes a poster to support brainstorm sessions, an interactive survey, and an accompanying guide with further explanations. On the toolkit&amp;rsquo;s website you can also find several case studies that highlight ethical issues in data projects, as well as a version of the toolkit specifically for researchers.</description>
    </item>
    
    <item>
      <title>DiCE: Diverse Counterfactual Explanations</title>
      <link>http://example.org/tools/dice/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/dice/</guid>
      <description>From README:
 DiCE implements counterfactual (CF) explanations that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., you would have received the loan if your income was higher by $10,000. In other words, it provides &amp;ldquo;what-if&amp;rdquo; explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.
 A main innovation of DiCE is that it implements a method to make producing counter-factual examples more model-agnostic:</description>
    </item>
    
    <item>
      <title>Fairlearn</title>
      <link>http://example.org/tools/fairlearn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/fairlearn/</guid>
      <description>The documentation of fairlearn is excellent and provides a good introduction to the topic of fairness in AI. It is emphasized that fairness algorithms are no plug-and-play technical solutions, but require serious thought about the context of the data and the problem at hand.
 Fairness is a fundamentally sociotechnical challenge and cannot be solved with technical tools alone. They may be helpful for certain tasks such as assessing unfairness through various metrics, or to mitigate observed unfairness when training a model.</description>
    </item>
    
    <item>
      <title>Fairness Decision Tree</title>
      <link>http://example.org/tools/fairness-tree/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/fairness-tree/</guid>
      <description>This fairness tree is shown in the web version of the Aequitasbias and fairness audit toolkit. It&amp;rsquo;s main purpose is to help decide on a suitable fairness metric, given the data set and the type of problem. Because this can also be useful to be used with other fairness toolkits, it merited its own entry.</description>
    </item>
    
    <item>
      <title>IBM AI Fairness 360</title>
      <link>http://example.org/tools/ibm-ai-fairness-360/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/ibm-ai-fairness-360/</guid>
      <description>The IBM AI Fairness 360 Toolkit contains several bias mitigation algorithms that are applicable to various stages of the machine learning pipeline. The toolkit implements different notions of fairness, both on individual and the group level, and several fairness metrics for both classes of fairness. The toolkit provides additional guidance on choosing metrics and mitigation algorithms given a particular goal and application.
The following should be noted when using the fairness toolkit (and other similar toolkits, for that matter):</description>
    </item>
    
    <item>
      <title>InterpretML</title>
      <link>http://example.org/tools/interpretml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/interpretml/</guid>
      <description>The InterpretML toolkit, developed at Microsoft, can be decomposed in two major components:
 A set of interpretable &amp;ldquo;glassbox&amp;rdquo; models Techniques for explaining black box systems.  W.r.t. 1, InterpretML particularly contains a new interpretable &amp;ldquo;glassbox&amp;rdquo; model that combines Generalized Additive Models (GAMs) with machine learning techniques such as gradient boosted trees, called an Explainable Boosting Machine.
Other than this new interpretable model, the main utility of InterpretML is to unify existing explainability techniques under a single API.</description>
    </item>
    
    <item>
      <title>LIME: Local Interpretable Model-agnostic Explanations</title>
      <link>http://example.org/tools/lime/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/lime/</guid>
      <description>The type of explanation LIME offers is a surrogate model that approximates a black box prediction locally. The surrogate model is a sparse linear model, which means that the surrogate model is interpretable (in this case, it&amp;rsquo;s weights are meaningful). This simpler model can thus help to explain the black box prediction, assuming the local approximation is actually representative.
The intuition behind this is provided in the README:
 Intuitively, an explanation is a local linear approximation of the model&amp;rsquo;s behaviour.</description>
    </item>
    
    <item>
      <title>Model cards for Model Reporting</title>
      <link>http://example.org/tools/model-card/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/model-card/</guid>
      <description>Model cards are an extension of the datasheetto machine learning models.
 Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains.</description>
    </item>
    
    <item>
      <title>SHAP: SHapley Additive exPlanations</title>
      <link>http://example.org/tools/shap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/shap/</guid>
      <description>The SHAP package is built on the concept of a Shapley valueand can generate explanations model-agnostically. So it only requires input and output values, not model internals:
 SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. (README)
 Additionally, this package also contains several model-specific implementations of Shapley values that are optimized for a particular machine learning model and sometimes even for a particular library.</description>
    </item>
    
    <item>
      <title>SMACTR: End-to-End Framework for Internal Algorithmic Auditing</title>
      <link>http://example.org/tools/smactr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/smactr/</guid>
      <description>Introduction A major downside of external auditing is that it typically only can be done after model deployment. This paper presents a methodology for internal algorithmic auditing as an integral part of the development process, end-to-end.
Those who move fast and break things, beware:
 The audit process is necessarily boring, slow, meticulous and methodical—antithetical to the typical rapid development pace for AI technology. However, it is critical to slow down as algorithms continue to be deployed in increasingly high-stakes domains.</description>
    </item>
    
  </channel>
</rss>