<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>classification</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	<link rel="alternate" type="application/rss+xml" href="/tasks/classification/index.xml" title="Ethical AI Tool Landscape">
</head>
<body>
	<header>
    <div class='inline-block'>
        <a href="https://github.com/EdwinWenink/ai-ethics-tool-landscape"><img src='/img/GitHub-Mark/PNG/GitHub-Mark-32px.png' alt="Github page" class='inline-logo'></a>
    </div>
	<nav>
        <a href="http://example.org/">Ethical AI Tool Landscape</a>
        
                
                > <a href="/values/">Values</a> 
                
                > <a href="/stages/">Stages</a> 
                
                > <a href="/categories/">Categories</a> 
                
                > <a href="/tools/">Tools</a> 
                
                > <a href="/taxonomy/">Taxonomy</a> 
                
        
	</nav>
</header>

	
	<main>
		
		<div>
			<h1 class="title">classification</h1>
			
		</div>
		

        
            <article>
	<h1><a href="http://example.org/tools/aequitas/">Aequitas: Bias and Fairness Audit Toolkit</a></h1>
    
	
	<div>
		Audit The Aequitas toolkit can both be used on the command-line, programmatically via its Python API or via a web interface. The web interface offers a four step programme to audit a dataset on bias. The four steps are:
 Upload (tabular) data Determine protected groups and reference group Select fairness metrics and disparity intolerance Inspect bias report  Example audit report.    This toolkit is useful for auditing bias and fairness according to a limited set of common fairness metrics, but does not offer algorithms for mitigating bias.
		
			<a href="http://example.org/tools/aequitas/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/art/">ART: Adversial Robustness 360 Toolbox</a></h1>
    
	
	<div>
		The Adversial Robustness Toolbox (ART) is the first comprehensive toolbox that unifies many defensive techniques for four categories of adversial attacks on machine learning models. These categories are model evasion, model poisoning, model extraction and inference (e.g. inference of sensitive attributes in the training data; or determining whether an example was part of the training data). ART supports all popular machine learning frameworks, all data types and all machine learning tasks.
		
			<a href="http://example.org/tools/art/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/dice/">DiCE: Diverse Counterfactual Explanations</a></h1>
    
	
	<div>
		From README:
 DiCE implements counterfactual (CF) explanations that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., you would have received the loan if your income was higher by $10,000. In other words, it provides &ldquo;what-if&rdquo; explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.
 A main innovation of DiCE is that it implements a method to make producing counter-factual examples more model-agnostic:
		
			<a href="http://example.org/tools/dice/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/fairlearn/">Fairlearn</a></h1>
    
	
	<div>
		The documentation of fairlearn is excellent and provides a good introduction to the topic of fairness in AI. It is emphasized that fairness algorithms are no plug-and-play technical solutions, but require serious thought about the context of the data and the problem at hand.
 Fairness is a fundamentally sociotechnical challenge and cannot be solved with technical tools alone. They may be helpful for certain tasks such as assessing unfairness through various metrics, or to mitigate observed unfairness when training a model.
		
			<a href="http://example.org/tools/fairlearn/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/ibm-ai-fairness-360/">IBM AI Fairness 360</a></h1>
    
	
	<div>
		The IBM AI Fairness 360 Toolkit contains several bias mitigation algorithms that are applicable to various stages of the machine learning pipeline. The toolkit implements different notions of fairness, both on individual and the group level, and several fairness metrics for both classes of fairness. The toolkit provides additional guidance on choosing metrics and mitigation algorithms given a particular goal and application.
The following should be noted when using the fairness toolkit (and other similar toolkits, for that matter):
		
			<a href="http://example.org/tools/ibm-ai-fairness-360/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/interpret-text/">Interpret-Text</a></h1>
    
	
	<div>
		Interpret-Text is an extension of InterpretML, specifically for several text models. Three modules are provided: ClassicalTextExplainer, UnifiedInformationExplainer and IntrospectiveRationaleExplainer.
Classical Text Explainer The ClassicalTextExplainer supports linear models from sklearn with a coefs_ call and tree-based models for which feature_importances_ is defined.
ClassicalTextExplainer includes a NLP pipeline from preprocessing to hyperparameter tuning, so it accepts raw text data as input. The default pipeline uses a unigram bag-of-words model. Elements of the pipeline can be replaced if desired.
		
			<a href="http://example.org/tools/interpret-text/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/interpretml/">InterpretML</a></h1>
    
	
	<div>
		The InterpretML toolkit, developed at Microsoft, can be decomposed in two major components:
 A set of interpretable &ldquo;glassbox&rdquo; models Techniques for explaining black box systems.  W.r.t. 1, InterpretML particularly contains a new interpretable &ldquo;glassbox&rdquo; model that combines Generalized Additive Models (GAMs) with machine learning techniques such as gradient boosted trees, called an Explainable Boosting Machine.
Other than this new interpretable model, the main utility of InterpretML is to unify existing explainability techniques under a single API.
		
			<a href="http://example.org/tools/interpretml/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/lime/">LIME: Local Interpretable Model-agnostic Explanations</a></h1>
    
	
	<div>
		The type of explanation LIME offers is a surrogate model that approximates a black box prediction locally. The surrogate model is a sparse linear model, which means that the surrogate model is interpretable (in this case, it&rsquo;s weights are meaningful). This simpler model can thus help to explain the black box prediction, assuming the local approximation is actually representative.
The intuition behind this is provided in the README:
 Intuitively, an explanation is a local linear approximation of the model&rsquo;s behaviour.
		
			<a href="http://example.org/tools/lime/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/shap/">SHAP: SHapley Additive exPlanations</a></h1>
    
	
	<div>
		The SHAP package is built on the concept of a Shapley valueand can generate explanations model-agnostically. So it only requires input and output values, not model internals:
 SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. (README)
 Additionally, this package also contains several model-specific implementations of Shapley values that are optimized for a particular machine learning model and sometimes even for a particular library.
		
			<a href="http://example.org/tools/shap/">Read more...</a>
		
	</div>
</article>

        
        
	</main>
    

	<footer>
    Last modified on 03-06-2021.
    <p>&copy; 2021 <a href="http://example.org/">Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>

</body>
</html>
