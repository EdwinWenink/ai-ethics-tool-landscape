<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>natural language processing on Ethical AI Tool Landscape</title>
    <link>http://example.org/tasks/natural-language-processing/</link>
    <description>Recent content in natural language processing on Ethical AI Tool Landscape</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Edwin Wenink</copyright>
    
	<atom:link href="http://example.org/tasks/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Debiaswe: try to make word embeddings less sexist</title>
      <link>http://example.org/tools/debiaswe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/debiaswe/</guid>
      <description>Word embeddings are a widely used representation for text data. A well-known example in natural language processing (NLP) is Word2vec, which uses a neural network to learn latent vector representations of words. It turns out that relations in this latent vector space capture semantic relations quite well. For example, by finding similar vectors you typically end up with highly related or synonymous words. Another typical example is that when you add up the vectors of &amp;ldquo;king&amp;rdquo; and &amp;ldquo;woman&amp;rdquo;, you end up with the vector corresponding to &amp;ldquo;queen&amp;rdquo;, so even a form of conceptual calculus is possible.</description>
    </item>
    
  </channel>
</rss>