<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Group fairness on Ethical AI Tool Landscape</title>
    <link>http://example.org/fairness/group-fairness/</link>
    <description>Recent content in Group fairness on Ethical AI Tool Landscape</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Edwin Wenink</copyright>
    
	<atom:link href="http://example.org/fairness/group-fairness/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Aequitas: Bias and Fairness Audit Toolkit</title>
      <link>http://example.org/tools/aequitas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/aequitas/</guid>
      <description>Audit The Aequitas toolkit can both be used on the command-line, programmatically via its Python API or via a web interface. The web interface offers a four step programme to audit a dataset on bias. The four steps are:
 Upload (tabular) data Determine protected groups and reference group Select fairness metrics and disparity intolerance Inspect bias report  Example audit report.    This toolkit is useful for auditing bias and fairness according to a limited set of common fairness metrics, but does not offer algorithms for mitigating bias.</description>
    </item>
    
    <item>
      <title>Debiaswe: try to make word embeddings less sexist</title>
      <link>http://example.org/tools/debiaswe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/debiaswe/</guid>
      <description>Word embeddings are a widely used representation for text data. A well-known example in natural language processing (NLP) is Word2vec, which uses a neural network to learn latent vector representations of words. It turns out that relations in this latent vector space capture semantic relations quite well. For example, by finding similar vectors you typically end up with highly related or synonymous words. Another typical example is that when you add up the vectors of &amp;ldquo;king&amp;rdquo; and &amp;ldquo;woman&amp;rdquo;, you end up with the vector corresponding to &amp;ldquo;queen&amp;rdquo;, so even a form of conceptual calculus is possible.</description>
    </item>
    
    <item>
      <title>Fairlearn</title>
      <link>http://example.org/tools/fairlearn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/fairlearn/</guid>
      <description>The documentation of fairlearn is excellent and provides a good introduction to the topic of fairness in AI. It is emphasized that fairness algorithms are no plug-and-play technical solutions, but require serious thought about the context of the data and the problem at hand.
 Fairness is a fundamentally sociotechnical challenge and cannot be solved with technical tools alone. They may be helpful for certain tasks such as assessing unfairness through various metrics, or to mitigate observed unfairness when training a model.</description>
    </item>
    
    <item>
      <title>IBM AI Fairness 360</title>
      <link>http://example.org/tools/ibm-ai-fairness-360/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/ibm-ai-fairness-360/</guid>
      <description>The IBM AI Fairness 360 Toolkit contains several bias mitigation algorithms that are applicable to various stages of the machine learning pipeline. The toolkit implements different notions of fairness, both on individual and the group level, and several fairness metrics for both classes of fairness. The toolkit provides additional guidance on choosing metrics and mitigation algorithms given a particular goal and application.
The following should be noted when using the fairness toolkit (and other similar toolkits, for that matter):</description>
    </item>
    
  </channel>
</rss>