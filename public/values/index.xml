<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Values on Ethical AI Tool Landscape</title>
    <link>http://example.org/values/</link>
    <description>Recent content in Values on Ethical AI Tool Landscape</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Edwin Wenink</copyright>
    
	<atom:link href="http://example.org/values/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Accountability</title>
      <link>http://example.org/values/accountability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/values/accountability/</guid>
      <description>Unlike explainabilityor fairness, most available tools for accountability are not technical solutions, but rather methods describing best practices. For the sake of simplicity tools for organizational transparency, as opposed to algorithmic interpretability, are also categorized under accountability.</description>
    </item>
    
    <item>
      <title>Explainability</title>
      <link>http://example.org/values/explainability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/values/explainability/</guid>
      <description>Explainability is instrumental for maintaining other values such as fairnessand for trust in AI systems. There is little consensus about what &amp;ldquo;explainability&amp;rdquo; precisely is. The related concepts of &amp;ldquo;transparency&amp;rdquo; and &amp;ldquo;interpretability&amp;rdquo; are sometimes used as synonyms, sometimes distinctly. For example, the explainability of machine learning models can be seen as one aspect of the overall need to be transparent in the use of AI (so transparency is the superconcept). But one may also use the word &amp;ldquo;transparency&amp;rdquo; to indicate &amp;ldquo;white box&amp;rdquo; models that are in themselves interpretable.</description>
    </item>
    
    <item>
      <title>Fairness</title>
      <link>http://example.org/values/fairness/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/values/fairness/</guid>
      <description>When trying to operationalize fairness it is important to realize that fairness in machine learning is a complex socio-technical issue. At minimum, this means that fairness tools should never be seen as plug-and-play solutions. This is already evident from the fact that - as most of the listed tools will emphasize - choices have to be made in which type of fairness is strived for. One general distinction for example is between group fairness and individual fairness.</description>
    </item>
    
    <item>
      <title>privacy</title>
      <link>http://example.org/values/privacy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/values/privacy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>safety</title>
      <link>http://example.org/values/safety/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/values/safety/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>