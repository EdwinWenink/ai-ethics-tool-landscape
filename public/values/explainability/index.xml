<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Explainability on Ethical AI Tool Landscape</title>
    <link>http://example.org/values/explainability/</link>
    <description>Recent content in Explainability on Ethical AI Tool Landscape</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Edwin Wenink</copyright>
    
	<atom:link href="http://example.org/values/explainability/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Agile Ethics for AI</title>
      <link>http://example.org/tools/agile_ethics_for_ai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/agile_ethics_for_ai/</guid>
      <description>Butnaru and others associated with the HAI center at Stanford set up a Agile Ethics workflow in the form of a Trello board. From left to right, the workflow walks you through relevant ethical considerations at the various steps of a machine learning pipeline. The phases are:
 Scope  Consider ethical implications of the project Consider skill mapping (what&amp;rsquo;s the impact of AI on jobs)?  Facilitates up-skilling or a change of strategy in the use of human talent     Data audit  Led by Chief Data Officer &amp;ldquo;Meet and plan&amp;rdquo; stage in Agile Helpful: Data Ethics Canvas   Train  Build stage in Agile Consider (tools for) transparency and fairness   Analyse  Benchmarks, including benchmarks related to e.</description>
    </item>
    
    <item>
      <title>AI Ethics Guidelines Global Inventory</title>
      <link>http://example.org/tools/algorithmwatch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/algorithmwatch/</guid>
      <description>AlgorithmWatch is maintaining a searchable inventory of published frameworks that set out ethical AI values. They can be searched on sector/actor, type, region and location.
AlgorithmWatch noted some common patterns here after publishing the first version of the index:
 &amp;ldquo;All include the similar principles on transparency, equality/non-discrimination, accountability and safety. Some add additional principles, such as the demand for AI be socially beneficial and protect human rights.&amp;rdquo; &amp;ldquo;Most frameworks are developed by coalitions, or institutions such as universities that then invite companies and individuals to sign up to these.</description>
    </item>
    
    <item>
      <title>Data Ethics Canvas</title>
      <link>http://example.org/tools/data-ethics-canvas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/data-ethics-canvas/</guid>
      <description>The Data Ethics Canvas is a tool developed by the Open Data Institute for providing ethical guidance to organizations doing any type of project involving data. That includes data collection, sharing, and its usage for example in machine learning applications. The tool is accompanied with a white paper and a brief practical guide for its usage.
Page 3 of the practical guide lists some recommendations that are also relevant when you do not use this tool.</description>
    </item>
    
    <item>
      <title>DiCE: Diverse Counterfactual Explanations</title>
      <link>http://example.org/tools/dice/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/dice/</guid>
      <description>From README:
 DiCE implements counterfactual (CF) explanations that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., you would have received the loan if your income was higher by $10,000. In other words, it provides &amp;ldquo;what-if&amp;rdquo; explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.
 A main innovation of DiCE is that it implements a method to make producing counter-factual examples more model-agnostic:</description>
    </item>
    
    <item>
      <title>Interpret-Text</title>
      <link>http://example.org/tools/interpret-text/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/interpret-text/</guid>
      <description>Interpret-Text is an extension of InterpretML, specifically for several text models. Three modules are provided: ClassicalTextExplainer, UnifiedInformationExplainer and IntrospectiveRationaleExplainer.
Classical Text Explainer The ClassicalTextExplainer supports linear models from sklearn with a coefs_ call and tree-based models for which feature_importances_ is defined.
ClassicalTextExplainer includes a NLP pipeline from preprocessing to hyperparameter tuning, so it accepts raw text data as input. The default pipeline uses a unigram bag-of-words model. Elements of the pipeline can be replaced if desired.</description>
    </item>
    
    <item>
      <title>InterpretML</title>
      <link>http://example.org/tools/interpretml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/interpretml/</guid>
      <description>The InterpretML toolkit, developed at Microsoft, can be decomposed in two major components:
 A set of interpretable &amp;ldquo;glassbox&amp;rdquo; models Techniques for explaining black box systems.  W.r.t. 1, InterpretML particularly contains a new interpretable &amp;ldquo;glassbox&amp;rdquo; model that combines Generalized Additive Models (GAMs) with machine learning techniques such as gradient boosted trees, called an Explainable Boosting Machine.
Other than this new interpretable model, the main utility of InterpretML is to unify existing explainability techniques under a single API.</description>
    </item>
    
    <item>
      <title>LIME: Local Interpretable Model-agnostic Explanations</title>
      <link>http://example.org/tools/lime/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/lime/</guid>
      <description>The type of explanation LIME offers is a surrogate model that approximates a black box prediction locally. The surrogate model is a sparse linear model, which means that the surrogate model is interpretable (in this case, it&amp;rsquo;s weights are meaningful). This simpler model can thus help to explain the black box prediction, assuming the local approximation is actually representative.
The intuition behind this is provided in the README:
 Intuitively, an explanation is a local linear approximation of the model&amp;rsquo;s behaviour.</description>
    </item>
    
    <item>
      <title>SHAP: SHapley Additive exPlanations</title>
      <link>http://example.org/tools/shap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/shap/</guid>
      <description>The SHAP package is built on the concept of a Shapley valueand can generate explanations model-agnostically. So it only requires input and output values, not model internals:
 SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. (README)
 Additionally, this package also contains several model-specific implementations of Shapley values that are optimized for a particular machine learning model and sometimes even for a particular library.</description>
    </item>
    
  </channel>
</rss>