<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>safety</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	<link rel="alternate" type="application/rss+xml" href="/values/safety/index.xml" title="Ethical AI Tool Landscape">
</head>
<body>
	<header>
    <div class='inline-block'>
        <a href="https://github.com/EdwinWenink/ai-ethics-tool-landscape"><img src='/img/GitHub-Mark/PNG/GitHub-Mark-32px.png' alt="Github page" class='inline-logo'></a>
    </div>
	<nav>
        <a href="http://example.org/">Ethical AI Tool Landscape</a>
        
                
                > <a href="/values/">Values</a> 
                
                > <a href="/stages/">Stages</a> 
                
                > <a href="/categories/">Categories</a> 
                
                > <a href="/tools/">Tools</a> 
                
                > <a href="/taxonomy/">Taxonomy</a> 
                
        
	</nav>
</header>

	
	<main>
		
		<div>
			<h1 class="title">safety</h1>
			
		</div>
		

        
            <article>
	<h1><a href="http://example.org/tools/algorithmwatch/">AI Ethics Guidelines Global Inventory</a></h1>
    
	
	<div>
		AlgorithmWatch is maintaining a searchable inventory of published frameworks that set out ethical AI values. They can be searched on sector/actor, type, region and location.
AlgorithmWatch noted some common patterns here after publishing the first version of the index:
 &ldquo;All include the similar principles on transparency, equality/non-discrimination, accountability and safety. Some add additional principles, such as the demand for AI be socially beneficial and protect human rights.&rdquo; &ldquo;Most frameworks are developed by coalitions, or institutions such as universities that then invite companies and individuals to sign up to these.
		
			<a href="http://example.org/tools/algorithmwatch/">Read more...</a>
		
	</div>
</article>

        
            <article>
	<h1><a href="http://example.org/tools/art/">ART: Adversial Robustness 360 Toolbox</a></h1>
    
	
	<div>
		The Adversial Robustness Toolbox (ART) is the first comprehensive toolbox that unifies many defensive techniques for four categories of adversial attacks on machine learning models. These categories are model evasion, model poisoning, model extraction and inference (e.g. inference of sensitive attributes in the training data; or determining whether an example was part of the training data). ART supports all popular machine learning frameworks, all data types and all machine learning tasks.
		
			<a href="http://example.org/tools/art/">Read more...</a>
		
	</div>
</article>

        
        
	</main>
    

	<footer>
    Last modified on 27-05-2021.
    <p>&copy; 2021 <a href="http://example.org/">Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>

</body>
</html>
