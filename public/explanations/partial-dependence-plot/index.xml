<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>partial dependence plot on Ethical AI Tool Landscape</title>
    <link>http://example.org/explanations/partial-dependence-plot/</link>
    <description>Recent content in partial dependence plot on Ethical AI Tool Landscape</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Edwin Wenink</copyright>
    
	<atom:link href="http://example.org/explanations/partial-dependence-plot/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>InterpretML</title>
      <link>http://example.org/tools/interpretml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/tools/interpretml/</guid>
      <description>The InterpretML toolkit, developed at Microsoft, can be decomposed in two major components:
 A set of interpretable &amp;ldquo;glassbox&amp;rdquo; models Techniques for explaining black box systems.  W.r.t. 1, InterpretML particularly contains a new interpretable &amp;ldquo;glassbox&amp;rdquo; model that combines Generalized Additive Models (GAMs) with machine learning techniques such as gradient boosted trees, called an Explainable Boosting Machine.
Other than this new interpretable model, the main utility of InterpretML is to unify existing explainability techniques under a single API.</description>
    </item>
    
  </channel>
</rss>