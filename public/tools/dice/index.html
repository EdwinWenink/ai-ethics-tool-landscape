<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>DiCE: Diverse Counterfactual Explanations</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
</head>
<body>
	<header>
    <div class='inline-block'>
        <a href="https://github.com/EdwinWenink/ai-ethics-tool-landscape"><img src='/img/GitHub-Mark/PNG/GitHub-Mark-32px.png' alt="Github page" class='inline-logo'></a>
    </div>
	<nav>
        <a href="http://example.org/">Ethical AI Tool Landscape</a>
        
                
                > <a href="/values/">Values</a> 
                
                > <a href="/stages/">Stages</a> 
                
                > <a href="/categories/">Categories</a> 
                
                > <a href="/tools/">Tools</a> 
                
                > <a href="/taxonomy/">Taxonomy</a> 
                
        
	</nav>
</header>

	
	<main>
		<article>
			<h1 class="title">DiCE: Diverse Counterfactual Explanations</h1>
            

			<div>

            

            <ul>
                
                <li id="values"> Values: { 
                    
                        <a href="/values/explainability">explainability</a>
                     } 
                 

                    
                    
                    <ul>
                        <li>Explanation type: { <a href="/explanations/example-based">example-based</a>  }</li>
                    </ul>
                    

                    
                    
                </li> 

                
                <li id="categories"> Categories: {  <a href="/categories/model-agnostic">model-agnostic</a> <a href="/categories/model-specific">model-specific</a> } </li>
                 

                
                <li id="design-phase"> Design stage: {  <a href="/stages/post-hoc">post-hoc</a> }</li> 
                 

                
                <li id="repository"> Repository: <a href="https://github.com/interpretml/DiCE" target="_blank">https://github.com/interpretml/DiCE</a></li>
                 

                
                <li id="model">Tasks: {  <a href="/tasks/classification">classification</a> <a href="/tasks/regression">regression</a> }</li> 
                 

                
                <li id="model">Input data: {  <a href="/data/tabular">tabular</a> }</li> 
                 

                
                <li id="licence"> Licence: MIT</li>
                 

                
                <li id="languages"> Languages: { <a href="/languages/python">Python</a>  }</li>
                 

                
                <li id="references">References: 
                    
                    
                        
                        <ul> 
                            <li><a href="https://arxiv.org/abs/1905.07697">Mothilal et al. - Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations</a></li>
                        </ul>
                        
                    
                    
                        
                        <ul> 
                            <li><a href="https://arxiv.org/abs/1912.03277">Mahajan et al. - Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers</a></li>
                        </ul>
                        
                    
                    
                        
                        <ul> 
                            <li><a href="https://arxiv.org/abs/1711.00399">Wachter et al. - Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR</a></li>
                        </ul>
                        
                     
                </li>
                 

                

            </ul>
			</div>

            <hr>

			<div>
				<p>From README:</p>
<blockquote>
<p>DiCE implements counterfactual (CF) explanations that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., you would have received the loan if your income was higher by $10,000. In other words, it provides &ldquo;what-if&rdquo; explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.</p>
</blockquote>
<p>A main innovation of <code>DiCE</code> is that it implements a method to make producing counter-factual examples more model-agnostic:</p>
<blockquote>
<p>Barring simple linear models, however, it is difficult to generate CF examples that work for any machine learning model. DiCE is based on recent research that generates CF explanations for any ML model. The core idea is to setup finding such explanations as an optimization problem, similar to finding adversarial examples.</p>
</blockquote>
<p><code>DiCe</code> supports various 

<a href="/categories/model-agnostic/">model-agnostic</a>
 methods to find counterfactual examples.</p>
<ul>
<li>Randomized sampling</li>
<li>KD-tree algorithm</li>
<li>Genetic algorithm</li>
</ul>
<p>Additionally, <em>gradient-based methods</em> are provided for <em>differentiable</em> models (e.g. a neural network).</p>
<ul>
<li>Loss-based method from Mothilal et al. (2020)</li>
<li>Method based on a variational auto-encoder, Mahajan et al. (2019)</li>
</ul>
<p>An interesting detail is that DiCE does not necessarily need access to the full data set, so it is possible to generate counterfactuals while keeping the data private.</p>

			</div>
		</article>
	</main>
    

	<footer>
    Last modified on 20-05-2021.
    <p>&copy; 2021 <a href="http://example.org/">Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>

</body>
</html>
