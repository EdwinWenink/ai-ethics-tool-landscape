<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Values on Ethical AI Tool Landscape</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/</link><description>Recent content in Values on Ethical AI Tool Landscape</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Edwin Wenink</copyright><atom:link href="https://edwinwenink.github.io/ai-ethics-tool-landscape/values/index.xml" rel="self" type="application/rss+xml"/><item><title>Accountability</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/accountability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/accountability/</guid><description>Unlike explainability or fairness , most available tools for accountability are not technical solutions, but rather methods describing best practices. For the sake of simplicity tools for organizational transparency, as opposed to algorithmic interpretability, are also categorized under accountability.</description></item><item><title>Explainability</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/explainability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/explainability/</guid><description>Explainability is instrumental for maintaining other values such as fairness and for trust in AI systems. There is little consensus about what &amp;ldquo;explainability&amp;rdquo; precisely is. The related concepts of &amp;ldquo;transparency&amp;rdquo; and &amp;ldquo;interpretability&amp;rdquo; are sometimes used as synonyms, sometimes distinctly. For example, the explainability of machine learning models can be seen as one aspect of the overall need to be transparent in the use of AI (so transparency is the superconcept). But one may also use the word &amp;ldquo;transparency&amp;rdquo; to indicate &amp;ldquo;white box&amp;rdquo; models that are in themselves interpretable.</description></item><item><title>Fairness</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness/</guid><description>When trying to operationalize fairness it is important to realize that fairness in machine learning is a complex socio-technical issue. At minimum, this means that fairness tools should never be seen as plug-and-play solutions. This is already evident from the fact that - as most of the listed tools will emphasize - choices have to be made in which type of fairness is strived for. One general distinction for example is between group fairness and individual fairness.</description></item><item><title>Privacy</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/privacy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/privacy/</guid><description>Most AI applications nowadays rely on large amounts of data, and regulations like the European Data Protection Regulation (GDPR) have to be taken into account. Tools for privacy in AI can for example be practical guidelines for implementing privacy by design into AI applications, or technical methods to prevent extraction of privacy-sensitive information from trained models.</description></item><item><title>Security</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security/</guid><description>Security focuses on tools for mitigating AI-specific risks, such as tools that promote robustness against adversarial attacks.
For example, it is possible to create adversarial examples by finding perturbations that maximize the prediction error and then applying these perturbations to input images in such a way that 1) a human does not or barely see the difference but 2) the neural network misclassifies the input. If the classifications of the neural network have impact, then this type of attack needs to be countered for safe usage of the model.</description></item></channel></rss>