<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<link rel="shortcut icon" href=img/favicon.ico>
<title>Security</title>
<link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css>
<link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security/index.xml title="Ethical AI Tool Landscape">
</head>
<body>
<header>
<div class=inline-block>
<a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a>
</div>
<nav>
<div id=breadcrumbs>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values>Values</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security/>Security</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a>
</div>
</nav>
</header>
<main>
<div>
<h1 class=title>Security</h1>
<div><p>Security focuses on tools for mitigating AI-specific risks, such as tools that promote robustness against adversarial attacks.</p>
<p>For example, it is possible to create adversarial examples by finding perturbations that maximize the prediction error and then applying these perturbations to input images in such a way that 1) a human does not or barely see the difference but 2) the neural network misclassifies the input.
If the classifications of the neural network have impact, then this type of attack needs to be countered for safe usage of the model.</p>
<p>Security is important for the <em>safety</em> of AI, but safety may be construed as a broader category that also involves
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/explainability/>explainability</a>
or
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness/>fairness</a>
in the sense of &ldquo;protection against harm&rdquo;.</p>
</div>
</div>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/advbox/>AdvBox</a></h1>
<div>
Advbox offers a number of AI model security toolkits. AdversialBox allows zero-coding generation of adversial examples for a wide range of neural network frameworks. An overview of the supported attacks and defenses can be found here and the corresponding code here. It requires some effort to find all attacks mentioned on the homepage in the code base.
Generally speaking, the documentation of AdvBox is incomplete and not very user-friendly. ODD: Object Detector Deception showcases a specific attack for object detection networks such as YOLO, but is not mentioned in the README.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/advbox/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi-detect/>Alibi Detect</a></h1>
<div>
Alibi Detect is an open source Python library (sister library to Alibi ) focused detecting outliers, adversarial examples, and concept drift.
Finding adversarial examples is relevant for assessing the security of machine learning models. Machine learning models learn complex statistical patterns in datasets. If these statistical patterns &ldquo;drift&rdquo; (in unforeseen ways) after a model is deployed, this will decrease the model performance over time. In systems where model predictions have an impact on people, this may be a threat to the fairness of the predictions.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi-detect/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/art/>ART: Adversial Robustness 360 Toolbox</a></h1>
<div>
The Adversial Robustness Toolbox (ART) is the first comprehensive toolbox that unifies many defensive techniques for four categories of adversarial attacks on machine learning models. These categories are model evasion, model poisoning, model extraction and inference (e.g. inference of sensitive attributes in the training data; or determining whether an example was part of the training data). ART supports all popular machine learning frameworks, all data types and all machine learning tasks.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/art/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/foolbox/>Foolbox</a></h1>
<div>
Foolbox is a comprehensive adversarial library for attacking machine learning models, with a focus on neural networks in computer vision. At the moment of writing FoolBox contains 41 gradient-based and decision-based adversarial attacks, making it the second biggest adversial library after ART . A notable difference with ART is that Foolbox only contains attacks, but no defenses and evaluation metrics.
The library is very user-friendly, with a clear API and documentation.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/foolbox/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/openmined/>OpenMined (PySyft)</a></h1>
<div>
The OpenMined community is a collaboration of several organizations, including TensorFlow, PyTorch and Keras, to create an open-source ecosystem of privacy tools that extend libraries such as PyTorch with cryptographic techniques and differential privacy. The aim is to contribute to the adaptation of privacy-preserving AI.
To this end, OpenMined offers several privacy-preserving tools on their github. A main tool is PySyft, which allows &ldquo;computing on data you do not own and cannot see&rdquo;.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/openmined/>Read more...</a>
</div>
</article>
</main>
<footer>
<p>Last modified on 22-07-2021.</p>
<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>
</body>
</html>