<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Values</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css><link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/index.xml title="Ethical AI Tool Landscape"></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/>Values</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/>Stages</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/taxonomy/>Taxonomy</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/>Tools</a></nav></header><main><div><h1 class=title>Values</h1><div><p>The last few years many companies, institutions and governmental organizations have published documents stating principles for the responsible development of Artificial Intelligence.
These documents usually refer to the following common set of values to uphold:</p></div></div><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/accountability/>Accountability</a></h1><div>Unlike explainability or fairness , most available tools for accountability are not technical solutions, but rather methods describing best practices. For the sake of simplicity tools for organizational transparency, as opposed to algorithmic interpretability, are also categorized under accountability.</div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/explainability/>Explainability</a></h1><div>Explainability is instrumental for maintaining other values such as fairness and for trust in AI systems. There is little consensus about what &ldquo;explainability&rdquo; precisely is. The related concepts of &ldquo;transparency&rdquo; and &ldquo;interpretability&rdquo; are sometimes used as synonyms, sometimes distinctly. For example, the explainability of machine learning models can be seen as one aspect of the overall need to be transparent in the use of AI (so transparency is the superconcept). But one may also use the word &ldquo;transparency&rdquo; to indicate &ldquo;white box&rdquo; models that are in themselves interpretable.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/explainability/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness/>Fairness</a></h1><div>When trying to operationalize fairness it is important to realize that fairness in machine learning is a complex socio-technical issue. At minimum, this means that fairness tools should never be seen as plug-and-play solutions. This is already evident from the fact that - as most of the listed tools will emphasize - choices have to be made in which type of fairness is strived for. One general distinction for example is between group fairness and individual fairness.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/privacy/>Privacy</a></h1><div>Most AI applications nowadays rely on large amounts of data, and regulations like the European Data Protection Regulation (GDPR) have to be taken into account. Tools for privacy in AI can for example be practical guidelines for implementing privacy by design into AI applications, or technical methods to prevent extraction of privacy-sensitive information from trained models.</div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security/>Security</a></h1><div>Security focuses on tools for mitigating AI-specific risks, such as tools that promote robustness against adversial attacks. Security is important for the safety of AI, but safety may be construed as a broader category that also involves explainability or fairness in the sense of &ldquo;protection against harm&rdquo;.</div></article></main><footer>Last modified on 14-05-2021.<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>