<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Fairness</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css><link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness/index.xml title="Ethical AI Tool Landscape"></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/>Values</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/>Stages</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/taxonomy/>Taxonomy</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/>Tools</a></nav></header><main><div><h1 class=title>Fairness</h1><div><p>When trying to operationalize fairness it is important to realize that fairness in machine learning is a complex socio-technical issue.
At minimum, this means that fairness tools should never be seen as plug-and-play solutions.
This is already evident from the fact that - as most of the listed tools will emphasize - choices have to be made in which <em>type</em> of fairness is strived for.
One general distinction for example is between <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/group-fairness>group fairness</a> and <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/individual-fairness>individual fairness</a>.
Within group fairness, different parity metrics correspond to different &ldquo;worldviews&rdquo; on equality.</p><p>Additionally, there are different <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/>stages</a> in the machine learning pipeline where you can intervene.
The choice for a particular algorithm will partially be guided by the level of access to different stages of the machine learning pipeline.
Generally speaking, it is optimal to intervene as early as possible.
In particularly, bias mitigation in the
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing/>preprocessing</a>
stage can address both group- and individual fairness.</p><p>With all these factors combined, choosing an appropriate bias mitigation strategy is a complex tasks that requires both expertise in data science and sensitivity to context and values.</p></div></div><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/aequitas/>Aequitas: Bias and Fairness Audit Toolkit</a></h1><div>Audit The Aequitas toolkit can both be used on the command-line, programmatically via its Python API or via a web interface. The web interface offers a four step programme to audit a dataset on bias. The four steps are:
Upload (tabular) data Determine protected groups and reference group Select fairness metrics and disparity intolerance Inspect bias report Example audit report. This toolkit is useful for auditing bias and fairness according to a limited set of common fairness metrics, but does not offer algorithms for mitigating bias.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/aequitas/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/agile_ethics_for_ai/>Agile Ethics for AI</a></h1><div>Butnaru and others associated with the HAI center at Stanford set up a Agile Ethics workflow in the form of a Trello board. From left to right, the workflow walks you through relevant ethical considerations at the various steps of a machine learning pipeline. The phases are:
Scope Consider ethical implications of the project Consider skill mapping (what&rsquo;s the impact of AI on jobs)? Facilitates up-skilling or a change of strategy in the use of human talent Data audit Led by Chief Data Officer &ldquo;Meet and plan&rdquo; stage in Agile Helpful: Data Ethics Canvas Train Build stage in Agile Consider (tools for) transparency and fairness Analyse Benchmarks, including benchmarks related to e.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/agile_ethics_for_ai/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-ethics-canvas/>Data Ethics Canvas</a></h1><div>The Data Ethics Canvas is a tool developed by the Open Data Institute for providing ethical guidance to organizations doing any type of project involving data. That includes data collection, sharing, and its usage for example in machine learning applications. The tool is accompanied with a white paper and a brief practical guide for its usage.
Page 3 of the practical guide lists some recommendations that are also relevant when you do not use this tool.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-ethics-canvas/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/debiaswe/>Debiaswe: try to make word embeddings less sexist</a></h1><div>Word embeddings are a widely used representation for text data. A well-known example in natural language processing (NLP) is Word2vec, which uses a neural network to learn latent vector representations of words. It turns out that relations in this latent vector space capture semantic relations quite well. For example, by finding similar vectors you typically end up with highly related or synonymous words. Another typical example is that when you add up the vectors of &ldquo;king&rdquo; and &ldquo;woman&rdquo;, you end up with the vector corresponding to &ldquo;queen&rdquo;, so even a form of conceptual calculus is possible.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/debiaswe/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/equity-evaluation-corpus/>Equity Evaluation Corpus (EEC)</a></h1><div>This handcrafted dataset can be used to evaluate bias in AI using text data for NLP tasks. Dataset description:
Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems and resources. Further, there is a lack of benchmark datasets for examining inappropriate biases in system predictions. Here, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/equity-evaluation-corpus/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairlearn/>Fairlearn</a></h1><div>The documentation of fairlearn is excellent and provides a good introduction to the topic of fairness in AI. It is emphasized that fairness algorithms are no plug-and-play technical solutions, but require serious thought about the context of the data and the problem at hand.
Fairness is a fundamentally sociotechnical challenge and cannot be solved with technical tools alone. They may be helpful for certain tasks such as assessing unfairness through various metrics, or to mitigate observed unfairness when training a model.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairlearn/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ibm-ai-fairness-360/>IBM AI Fairness 360</a></h1><div>The IBM AI Fairness 360 Toolkit contains several bias mitigation algorithms that are applicable to various stages of the machine learning pipeline. The toolkit implements different notions of fairness, both on individual and the group level, and several fairness metrics for both classes of fairness. The toolkit provides additional guidance on choosing metrics and mitigation algorithms given a particular goal and application.
The following should be noted when using the fairness toolkit (and other similar toolkits, for that matter):
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ibm-ai-fairness-360/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/model-card/>Model cards for Model Reporting</a></h1><div>Model cards are an extension of the datasheet to machine learning models.
Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/model-card/>Read more...</a></div></article></main><footer>Last modified on 28-05-2021.<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>