<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>speech recognition on Ethical AI Tool Landscape</title><link>http://example.org/tasks/speech-recognition/</link><description>Recent content in speech recognition on Ethical AI Tool Landscape</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Edwin Wenink</copyright><atom:link href="http://example.org/tasks/speech-recognition/index.xml" rel="self" type="application/rss+xml"/><item><title>ART: Adversial Robustness 360 Toolbox</title><link>http://example.org/tools/art/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://example.org/tools/art/</guid><description>The Adversial Robustness Toolbox (ART) is the first comprehensive toolbox that unifies many defensive techniques for four categories of adversial attacks on machine learning models. These categories are model evasion, model poisoning, model extraction and inference (e.g. inference of sensitive attributes in the training data; or determining whether an example was part of the training data). ART supports all popular machine learning frameworks, all data types and all machine learning tasks.</description></item></channel></rss>