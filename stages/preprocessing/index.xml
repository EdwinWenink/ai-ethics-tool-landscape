<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Preprocessing on Ethical AI Tool Landscape</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing/</link><description>Recent content in Preprocessing on Ethical AI Tool Landscape</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Edwin Wenink</copyright><atom:link href="https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing/index.xml" rel="self" type="application/rss+xml"/><item><title>Aequitas: Bias and Fairness Audit Toolkit</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/aequitas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/aequitas/</guid><description>Audit The Aequitas toolkit can both be used on the command-line, programmatically via its Python API or via a web interface. The web interface offers a four step programme to audit a dataset on bias. The four steps are:
Upload (tabular) data Determine protected groups and reference group Select fairness metrics and disparity intolerance Inspect bias report Example audit report. This toolkit is useful for auditing bias and fairness according to a limited set of common fairness metrics, but does not offer algorithms for mitigating bias.</description></item><item><title>Agile Ethics for AI</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/agile-ethics-for-ai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/agile-ethics-for-ai/</guid><description>Butnaru and others associated with the HAI center at Stanford set up a Agile Ethics workflow in the form of a Trello board. From left to right, the workflow walks you through relevant ethical considerations at the various steps of a machine learning pipeline. The phases are:
Scope Consider ethical implications of the project Consider skill mapping (what&amp;rsquo;s the impact of AI on jobs)? Facilitates up-skilling or a change of strategy in the use of human talent Data audit Led by Chief Data Officer &amp;ldquo;Meet and plan&amp;rdquo; stage in Agile Helpful: Data Ethics Canvas Train Build stage in Agile Consider (tools for) transparency and fairness Analyse Benchmarks, including benchmarks related to e.</description></item><item><title>AI Fairness 360</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ai-fairness-360/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ai-fairness-360/</guid><description>The IBM AI Fairness 360 Toolkit contains several bias mitigation algorithms that are applicable to various stages of the machine learning pipeline. The toolkit implements different notions of fairness, both on individual and the group level, and several fairness metrics for both classes of fairness. The toolkit provides additional guidance on choosing metrics and mitigation algorithms given a particular goal and application.
The following should be noted when using the fairness toolkit (and other similar toolkits, for that matter):</description></item><item><title>ART: Adversial Robustness 360 Toolbox</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/art/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/art/</guid><description>The Adversial Robustness Toolbox (ART) is the first comprehensive toolbox that unifies many defensive techniques for four categories of adversarial attacks on machine learning models. These categories are model evasion, model poisoning, model extraction and inference (e.g. inference of sensitive attributes in the training data; or determining whether an example was part of the training data). ART supports all popular machine learning frameworks, all data types and all machine learning tasks.</description></item><item><title>Data Nutrition Label</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-nutrition-label/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-nutrition-label/</guid><description>In analogy with nutrition labels on food products, the authors of this paper propose a way to create a Data Nutrition Label. The goal of this method is to asses data quality and mitigate potential problems early on before building models on the data.
According to the authors, their approach is different from the datasheet in that the &amp;ldquo;proposed datasheet [i.e. by Gebru et al.] includes dataset provenance, key characteristics, relevant regulations and test results, but also significant yet more subjective information such as potential bias, strengths and weaknesses of the dataset, API, or model, and suggested uses.</description></item><item><title>Data Statements for NLP</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-statements-for-nlp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-statements-for-nlp/</guid><description>A data statement, according to the authors, is &amp;hellip;
a characterization of a dataset that provides context to allow developers and users to better understand how experimental results might generalize,how software might be appropriately deployed,and what biases might be reflected in systems built on the software. (587)
This paper specifically focuses on ethically responsive NLP technology. The authors argue that a data statement should be an integral part of work and writing on NLP.</description></item><item><title>Datasheets for Datasets</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/datasheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/datasheet/</guid><description>The method described in this paper aids in documenting datasets to help avoid unwanted consequences of data usage.
Abstract:
The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information.</description></item><item><title>Debiaswe: try to make word embeddings less sexist</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/debiaswe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/debiaswe/</guid><description>Word embeddings are a widely used representation for text data. A well-known example in natural language processing (NLP) is Word2vec, which uses a neural network to learn latent vector representations of words. It turns out that relations in this latent vector space capture semantic relations quite well. For example, by finding similar vectors you typically end up with highly related or synonymous words. Another typical example is that when you add up the vectors of &amp;ldquo;king&amp;rdquo; and &amp;ldquo;woman&amp;rdquo;, you end up with the vector corresponding to &amp;ldquo;queen&amp;rdquo;, so even a form of conceptual calculus is possible.</description></item><item><title>Fairlearn</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairlearn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairlearn/</guid><description>The documentation of fairlearn is excellent and provides a good introduction to the topic of fairness in AI. It is emphasized that fairness algorithms are no plug-and-play technical solutions, but require serious thought about the context of the data and the problem at hand.
Fairness is a fundamentally sociotechnical challenge and cannot be solved with technical tools alone. They may be helpful for certain tasks such as assessing unfairness through various metrics, or to mitigate observed unfairness when training a model.</description></item><item><title>Fairness Decision Tree</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairness-tree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairness-tree/</guid><description>This fairness tree is shown in the web version of the Aequitas bias and fairness audit toolkit. It&amp;rsquo;s main purpose is to help decide on a suitable fairness metric, given the data set and the type of problem. Because this can also be useful to be used with other fairness toolkits, it merited its own entry.</description></item><item><title>Model cards for Model Reporting</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/model-card/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/model-card/</guid><description>Model cards are an extension of the datasheet to machine learning models.
Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains.</description></item><item><title>SMACTR: End-to-End Framework for Internal Algorithmic Auditing</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/smactr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/smactr/</guid><description>Introduction A major downside of external auditing is that it typically only can be done after model deployment. This paper presents a methodology for internal algorithmic auditing as an integral part of the development process, end-to-end.
Those who move fast and break things, beware:
The audit process is necessarily boring, slow, meticulous and methodical—antithetical to the typical rapid development pace for AI technology. However, it is critical to slow down as algorithms continue to be deployed in increasingly high-stakes domains.</description></item><item><title>XAI Toolbox</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/xai-toolbox/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/xai-toolbox/</guid><description>This library is a small toolbox that offers some convenience functions for quickly visualizing imbalances in the data set, computing (permutation) feature importances and metrics such as the ROC-curve. A function to balance the data is offered through basic up- or downsampling, but other than this no fairness criteria are defined.
Compared to other libraries the XAI Toolbox is very basic and currently the roadmap (which is not updated since 2019) does not include any major improvements.</description></item></channel></rss>