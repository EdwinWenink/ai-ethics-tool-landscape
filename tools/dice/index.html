<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<link rel="shortcut icon" href=img/favicon.ico>
<title>DiCE: Diverse Counterfactual Explanations</title>
<link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css>
</head>
<body>
<header>
<div class=inline-block>
<a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a>
</div>
<nav>
<div id=breadcrumbs>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/dice/>Dice</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a>
</div>
</nav>
</header>
<main>
<article>
<h1 class=title>DiCE: Diverse Counterfactual Explanations</h1>
<div>
<ul>
<li id=values> Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/explainability>explainability</a>
}
<ul>
<li>Explanation type: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/example-based>example-based</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual>counterfactual</a> }</li>
</ul>
</li>
<li id=categories> Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic>model-agnostic</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-specific>model-specific</a> } </li>
<li id=design-phase> Stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/post-processing>post-processing</a> }</li>
<li id=repository> Repository: <a href=https://github.com/interpretml/DiCE target=_blank>https://github.com/interpretml/DiCE</a></li>
<li id=model>Tasks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/classification>classification</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/regression>regression</a> }</li>
<li id=model>Input data: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/tabular>tabular</a> }</li>
<li id=licence> Licence: MIT</li>
<li id=languages> Languages: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/python>Python</a> }</li>
<li id=references>References:
<ul>
<li><a href=https://arxiv.org/abs/1905.07697 target=_blank>Mothilal et al. - Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations</a></li>
</ul>
<ul>
<li><a href=https://arxiv.org/abs/1912.03277 target=_blank>Mahajan et al. - Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers</a></li>
</ul>
<ul>
<li><a href=https://arxiv.org/abs/1711.00399 target=_blank>Wachter et al. - Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR</a></li>
</ul>
</li>
</ul>
</div>
<hr>
<div>
<p>From README:</p>
<blockquote>
<p>DiCE implements counterfactual (CF) explanations that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., you would have received the loan if your income was higher by $10,000. In other words, it provides &ldquo;what-if&rdquo; explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.</p>
</blockquote>
<p>A main innovation of <code>DiCE</code> is that it implements a method to make producing counter-factual examples more model-agnostic:</p>
<blockquote>
<p>Barring simple linear models, however, it is difficult to generate CF examples that work for any machine learning model. DiCE is based on recent research that generates CF explanations for any ML model. The core idea is to setup finding such explanations as an optimization problem, similar to finding adversarial examples.</p>
</blockquote>
<p><code>DiCe</code> supports various
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic/>model-agnostic</a>
methods to find counterfactual examples.</p>
<ul>
<li>Randomized sampling</li>
<li>KD-tree algorithm</li>
<li>Genetic algorithm</li>
</ul>
<p>Additionally, <em>gradient-based methods</em> are provided for <em>differentiable</em> models (e.g. a neural network).</p>
<ul>
<li>Loss-based method from Mothilal et al. (2020)</li>
<li>Method based on a variational auto-encoder, Mahajan et al. (2019)</li>
</ul>
<p>An interesting detail is that DiCE does not necessarily need access to the full data set, so it is possible to generate counterfactuals while keeping the data private.</p>
</div>
</article>
</main>
<footer>
<p>Last modified on 13-07-2021.</p>
<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>
</body>
</html>