<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Alibi Detect</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><div id=breadcrumbs><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi-detect/>Alibi detect</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a></div></nav></header><main><article><h1 class=title>Alibi Detect</h1><div><ul><li id=values>Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security>security</a>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness>fairness</a>
}<ul><li>Fairness type: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/group-fairness>group fairness</a> }</li></ul></li><li id=categories>Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic>model-agnostic</a> }</li><li id=design-phase>Stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/design-phase>design phase</a> }</li><li id=repository>Repository: <a href=https://github.com/SeldonIO/alibi-detect target=_blank>https://github.com/SeldonIO/alibi-detect</a></li><li id=model>Tasks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/classification>classification</a> }</li><li id=model>Input data: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/text>text</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/image>image</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/tabular>tabular</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/time-series>time series</a> }</li><li id=licence>Licence: Apache License 2.0</li><li id=languages>Languages: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/python>Python</a> }</li><li id=framework>Frameworks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/frameworks/tensorflow>TensorFlow</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/frameworks/pytorch>PyTorch</a> }</li></ul></div><hr><div><p><em>Alibi Detect</em> is an open source Python library (sister library to
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Alibi</a>
) focused detecting outliers, adversial examples, and concept drift.</p><p>Finding adversial examples is relevant for assessing the
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security/>security</a>
of machine learning models.
Machine learning models learn complex statistical patterns in datasets.
If these statistical patterns &ldquo;drift&rdquo; (in unforeseen ways) after a model is deployed, this will decrease the model performance over time.
In systems where model predictions have an impact on people, this may be a threat to the
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness/>fairness</a>
of the predictions.</p><p>For each of these detection problems <em>Alibi Detect</em> supports a broad array of methods.
The README shows tables with these methods' characteristics, for <a href=https://github.com/SeldonIO/alibi-detect#outlier-detection>outlier detection</a>, <a href=https://github.com/SeldonIO/alibi-detect#adversarial-detection>adversial detection</a>, and <a href=https://github.com/SeldonIO/alibi-detect#drift-detection>drift detection</a>.
<a href=https://github.com/SeldonIO/alibi-detect#reference-list>Paper references</a> for all supported methods are also provided.</p><p>For drift detection methods, TensorFlow and PyTorch are supported.</p></div></article></main><footer><p>Last modified on 13-07-2021.</p><p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>