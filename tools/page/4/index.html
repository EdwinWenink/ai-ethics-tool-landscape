<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<link rel="shortcut icon" href=img/favicon.ico>
<title>Tools</title>
<link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css>
<link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/index.xml title="Ethical AI Tool Landscape">
</head>
<body>
<header>
<div class=inline-block>
<a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a>
</div>
<nav>
<div id=breadcrumbs>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/>Tools</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a>
</div>
</nav>
</header>
<main>
<div>
<h1 class=title>Tools</h1>
<div><p>This wiki focuses on practical tools for implementing ethical AI, but not all of these tools are strictly technical.
Particularly in the case of
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/accountability/>accountability</a>
, the listed tools can be methodologies or frameworks.</p>
<p>However, a prerequisite to be listed on this wiki is that some form of practical guidance must be offered.
In case of accountability, the tool may for example not be offered in a code repository, but instead in a paper that contains a practical component, e.g. a set of instructions, guidelines or guiding questions.</p>
</div>
</div>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/advbox/>AdvBox</a></h1>
<div>
Advbox offers a number of AI model security toolkits. AdversialBox allows zero-coding generation of adversial examples for a wide range of neural network frameworks. An overview of the supported attacks and defenses can be found here and the corresponding code here. It requires some effort to find all attacks mentioned on the homepage in the code base.
Generally speaking, the documentation of AdvBox is incomplete and not very user-friendly. ODD: Object Detector Deception showcases a specific attack for object detection networks such as YOLO, but is not mentioned in the README.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/advbox/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/aequitas/>Aequitas: Bias and Fairness Audit Toolkit</a></h1>
<div>
Audit The Aequitas toolkit can both be used on the command-line, programmatically via its Python API or via a web interface. The web interface offers a four step programme to audit a dataset on bias. The four steps are:
Upload (tabular) data Determine protected groups and reference group Select fairness metrics and disparity intolerance Inspect bias report Example audit report. This toolkit is useful for auditing bias and fairness according to a limited set of common fairness metrics, but does not offer algorithms for mitigating bias.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/aequitas/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/agile-ethics-for-ai/>Agile Ethics for AI</a></h1>
<div>
Butnaru and others associated with the HAI center at Stanford set up a Agile Ethics workflow in the form of a Trello board. From left to right, the workflow walks you through relevant ethical considerations at the various steps of a machine learning pipeline. The phases are:
Scope Consider ethical implications of the project Consider skill mapping (what&rsquo;s the impact of AI on jobs)? Facilitates up-skilling or a change of strategy in the use of human talent Data audit Led by Chief Data Officer &ldquo;Meet and plan&rdquo; stage in Agile Helpful: Data Ethics Canvas Train Build stage in Agile Consider (tools for) transparency and fairness Analyse Benchmarks, including benchmarks related to e.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/agile-ethics-for-ai/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/algorithmwatch/>AI Ethics Guidelines Global Inventory</a></h1>
<div>
AlgorithmWatch is maintaining a searchable inventory of published frameworks that set out ethical AI values. They can be searched on sector/actor, type, region and location.
AlgorithmWatch noted some common patterns here after publishing the first version of the index:
&ldquo;All include the similar principles on transparency, equality/non-discrimination, accountability and safety. Some add additional principles, such as the demand for AI be socially beneficial and protect human rights.&rdquo; &ldquo;Most frameworks are developed by coalitions, or institutions such as universities that then invite companies and individuals to sign up to these.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/algorithmwatch/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ai-explainability-360/>AI Explainability 360</a></h1>
<div>
The AI Explainability 360 (AIX360) toolkit is a Python library that offers a wide range of explanation types as well as some explainability metrics. AIX360 offers excellent guidance material, an interactive demo as well as developer tutorials. What&rsquo;s particularly good about this material is that it stimulates reflection on which type of explanation is appropriate, not only from a technical point of view, but also with respect to the target explainer and explainee.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ai-explainability-360/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ai-fairness-360/>AI Fairness 360</a></h1>
<div>
The IBM AI Fairness 360 Toolkit contains several bias mitigation algorithms that are applicable to various stages of the machine learning pipeline. The toolkit implements different notions of fairness, both on individual and the group level, and several fairness metrics for both classes of fairness. The toolkit provides additional guidance on choosing metrics and mitigation algorithms given a particular goal and application.
The following should be noted when using the fairness toolkit (and other similar toolkits, for that matter):
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ai-fairness-360/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/algorithmic-accountability-policy-toolkit/>Algorithmic Accountability Policy Toolkit</a></h1>
<div>
AI Now published the Algorithmic Accountability Policy Toolkit in 2018. It is specifically tailored towards advocates concerned with government use of algorithms. The toolkit provides a FAQ, an overview of various types of algorithms used by governments in specific application areas such as public health or criminal justice, and a comprehensive list of relevant literature.
The fact that this is a toolkit and not a paper can be seen from the very practical guidance that is offered.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/algorithmic-accountability-policy-toolkit/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Alibi</a></h1>
<div>
Alibi is an open-source Python library that supports various interpretability techniques and a broad array of explanation types. The README already provides an overview of the supported methods and when they are applicable. The following table with supported methods is copied from the README (slightly abbreviated):
Supported methods Method Models Explanations Classification Regression Tabular Text Images Categorical features ALE BB global ✔ ✔ ✔ Anchors BB local ✔ ✔ ✔ ✔ ✔ CEM BB* TF/Keras local ✔ ✔ ✔ Counterfactuals BB* TF/Keras local ✔ ✔ ✔ Prototype Counterfactuals BB* TF/Keras local ✔ ✔ ✔ ✔ Integrated Gradients TF/Keras local ✔ ✔ ✔ ✔ ✔ ✔ Kernel SHAP BB local global ✔ ✔ ✔ ✔ Tree SHAP WB local global ✔ ✔ ✔ ✔ The README also explains the keys:
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi-detect/>Alibi Detect</a></h1>
<div>
Alibi Detect is an open source Python library (sister library to Alibi ) focused detecting outliers, adversarial examples, and concept drift.
Finding adversarial examples is relevant for assessing the security of machine learning models. Machine learning models learn complex statistical patterns in datasets. If these statistical patterns &ldquo;drift&rdquo; (in unforeseen ways) after a model is deployed, this will decrease the model performance over time. In systems where model predictions have an impact on people, this may be a threat to the fairness of the predictions.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi-detect/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/art/>ART: Adversial Robustness 360 Toolbox</a></h1>
<div>
The Adversial Robustness Toolbox (ART) is the first comprehensive toolbox that unifies many defensive techniques for four categories of adversarial attacks on machine learning models. These categories are model evasion, model poisoning, model extraction and inference (e.g. inference of sensitive attributes in the training data; or determining whether an example was part of the training data). ART supports all popular machine learning frameworks, all data types and all machine learning tasks.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/art/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/captum/>Captum</a></h1>
<div>
Captum is a model interpretability library specifically PyTorch. It is actively maintained at the moment of writing and supports an extensive array of interpretability methods.
The Captum website also offers a large range of hands-on tutorials for various use cases.
Supported interpretability methods Captum supports a very extensive list of interpretability algorithms. All paper references for each of the supported methods are listed in the README, so they will not be repeated here.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/captum/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/cleverhans/>CleverHans</a></h1>
<div>
CleverHans is a Python library with the main purpose of providing good reference implementations of attacks for benchmarking machine learning models against adversarial examples. The main maintainers of this library are Ian Goodfellow and Nicolas Papernot. Attacks (i.e. methods for generating adversarial examples) are listed under /cleverhans and each of the supported frameworks has its own folder with attack implementations. CleverHans also aims to implement a set of defenses, but this is currently work in progress (currently there is only a defense implementation for PyTorch).
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/cleverhans/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/cem/>Contrastive Explanation Method (CEM)</a></h1>
<div>
Dhurandhar et al. support a type of contrastive explanation based on what they call pertinent negatives. A contrastive explanation answers the question: &ldquo;Why P, rather than Q&rdquo;?
CEM supports such an explanation by finding the minimal set of features that lead to prediction P (a pertinent positive that resembles an anchor explanation), and additionally a minimal set of features that should be absent to maintain decision P instead of the decision for closest class Q (a pertinent negative that is somewhat similar to a counterfactual ).
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/cem/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-ethics-canvas/>Data Ethics Canvas</a></h1>
<div>
The Data Ethics Canvas is a tool developed by the Open Data Institute for providing ethical guidance to organizations doing any type of project involving data. That includes data collection, sharing, and its usage for example in machine learning applications. The tool is accompanied with a white paper and a brief practical guide for its usage.
Page 3 of the practical guide lists some recommendations that are also relevant when you do not use this tool.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-ethics-canvas/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-nutrition-label/>Data Nutrition Label</a></h1>
<div>
In analogy with nutrition labels on food products, the authors of this paper propose a way to create a Data Nutrition Label. The goal of this method is to asses data quality and mitigate potential problems early on before building models on the data.
According to the authors, their approach is different from the datasheet in that the &ldquo;proposed datasheet [i.e. by Gebru et al.] includes dataset provenance, key characteristics, relevant regulations and test results, but also significant yet more subjective information such as potential bias, strengths and weaknesses of the dataset, API, or model, and suggested uses.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-nutrition-label/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-statements-for-nlp/>Data Statements for NLP</a></h1>
<div>
A data statement, according to the authors, is &mldr;
a characterization of a dataset that provides context to allow developers and users to better understand how experimental results might generalize,how software might be appropriately deployed,and what biases might be reflected in systems built on the software. (587)
This paper specifically focuses on ethically responsive NLP technology. The authors argue that a data statement should be an integral part of work and writing on NLP.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/data-statements-for-nlp/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/datasheet/>Datasheets for Datasets</a></h1>
<div>
The method described in this paper aids in documenting datasets to help avoid unwanted consequences of data usage.
Abstract:
The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/datasheet/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/debiaswe/>Debiaswe: try to make word embeddings less sexist</a></h1>
<div>
Word embeddings are a widely used representation for text data. A well-known example in natural language processing (NLP) is Word2vec, which uses a neural network to learn latent vector representations of words. It turns out that relations in this latent vector space capture semantic relations quite well. For example, by finding similar vectors you typically end up with highly related or synonymous words. Another typical example is that when you add up the vectors of &ldquo;king&rdquo; and &ldquo;woman&rdquo;, you end up with the vector corresponding to &ldquo;queen&rdquo;, so even a form of conceptual calculus is possible.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/debiaswe/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/deda/>DEDA: De Ethische Data Assistent</a></h1>
<div>
This toolkit developed by the Utrecht Data School supports data analysts, projectmanagers, and policy makers in identifying ethical values and issues in data projects and promoting accountability towards stakeholders. The toolkit is written in Dutch and includes a poster to support brainstorm sessions, an interactive survey, and an accompanying guide with further explanations. On the toolkit&rsquo;s website you can also find several case studies that highlight ethical issues in data projects, as well as a version of the toolkit specifically for researchers.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/deda/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/deepexplain/>DeepExplain</a></h1>
<div>
The DeepExplain Python package for TensorFlow models and Keras models with TensorFlow backend offers two types of interpretability methods for deep convolutional neural networks: gradient-based methods and perturbation-based methods. This package does not seem to be very actively maintained anymore and support for TensorFlow V2 is limited.
Attributions The README gives the following clear and succinct explanation of what an &ldquo;attribution&rdquo; is. All methods included in this approach allow visualization of how each input feature contributes to the final prediction, in terms of what a particular targeted neuron &ldquo;sees&rdquo;:
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/deepexplain/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/deeplift/>DeepLIFT</a></h1>
<div>
A brief explanation of the gradient-based interpretability method called DeepLIFT is given by Shrikumar et al. in the abstract of the linked paper:
DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its &lsquo;reference activation&rsquo; and assigns contribution scores according to the difference.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/deeplift/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/dice/>DiCE: Diverse Counterfactual Explanations</a></h1>
<div>
From README:
DiCE implements counterfactual (CF) explanations that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., you would have received the loan if your income was higher by $10,000. In other words, it provides &ldquo;what-if&rdquo; explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.
A main innovation of DiCE is that it implements a method to make producing counter-factual examples more model-agnostic:
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/dice/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/eli5/>ELI5</a></h1>
<div>
ELI5 (&ldquo;Explain Like I&rsquo;m 5&rdquo;) provides model-specific support for models from scikit-learn, lightning, decision tree ensembles using the xgboost, LightGBM, CatBoost libraries. ELI5 mainly provides convenient wrappers to couple the feature importance coefficients that these libraries already provide with feature names, as well as convenient ways to visualize importances, e.g. by highlighting words in a text. For Keras image classifiers an implementation of the gradient-based Grad-CAM visualizations is offered, but the TensorFlow V2 backend is not supported.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/eli5/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/equity-evaluation-corpus/>Equity Evaluation Corpus (EEC)</a></h1>
<div>
This handcrafted dataset can be used to evaluate bias in AI using text data for NLP tasks. Dataset description:
Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems and resources. Further, there is a lack of benchmark datasets for examining inappropriate biases in system predictions. Here, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/equity-evaluation-corpus/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/factsheets/>FactSheets: Increasing Trust in AI Services through Supplier's Declaration of Conformity</a></h1>
<div>
FactSheets, as proposed by Arnold et al. from IBM Research, are similar to the model-card and datasheet , but are significantly more comprehensive because they focus on whole AI services. A main difference is that live AI services may comprise several trained models that interact with each other. The model card and datasheet instead concern a single model and the data it is trained on. However, Arnold et al. point out that an AI service with safe components is not necessarily safe overall and &ldquo;so it is prudent to also consider transparency and accountability of services in addition to datasets and models&rdquo; (p.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/factsheets/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairlearn/>Fairlearn</a></h1>
<div>
The documentation of fairlearn is excellent and provides a good introduction to the topic of fairness in AI. It is emphasized that fairness algorithms are no plug-and-play technical solutions, but require serious thought about the context of the data and the problem at hand.
Fairness is a fundamentally sociotechnical challenge and cannot be solved with technical tools alone. They may be helpful for certain tasks such as assessing unfairness through various metrics, or to mitigate observed unfairness when training a model.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairlearn/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairness-tree/>Fairness Decision Tree</a></h1>
<div>
This fairness tree is shown in the web version of the Aequitas bias and fairness audit toolkit. It&rsquo;s main purpose is to help decide on a suitable fairness metric, given the data set and the type of problem. Because this can also be useful to be used with other fairness toolkits, it merited its own entry.
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairness-in-classification/>Fairness in Classification</a></h1>
<div>
The not-so originally named &ldquo;fairness in classification&rdquo; provides a Python implementation of three fairness constraints for logistic regression:
Disparate impact: similar acceptance rate for different demographic groups. See Zafar et al., 2017 a. Disparate mistreatment: similar misclassification rate for different demographic groups. See Zafar et al., 2017b Preference-based fairness (as opposed to parity-based fairness): a more game-theoretical approach where decision boundaries are chosen such that it can be shown that each group prefers its own decision boundary, if rational.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairness-in-classification/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/foolbox/>Foolbox</a></h1>
<div>
Foolbox is a comprehensive adversarial library for attacking machine learning models, with a focus on neural networks in computer vision. At the moment of writing FoolBox contains 41 gradient-based and decision-based adversarial attacks, making it the second biggest adversial library after ART . A notable difference with ART is that Foolbox only contains attacks, but no defenses and evaluation metrics.
The library is very user-friendly, with a clear API and documentation.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/foolbox/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/h2o-mli-resources/>H2O MLI Resources</a></h1>
<div>
This repository by H2O.ai contains useful resources and notebooks that showcase well-known machine learning interpretability techniques. The examples use the h2o Python package with their own estimators (e.g. their own fork of XGBoost), but all code is open-source and the examples are still illustrative of the interpretability techniques. These case studies that also deal with practical coding issues and preprocessing steps, e.g. that LIME can be unstable when there are strong correlations between input variables.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/h2o-mli-resources/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/interpret-text/>Interpret-Text</a></h1>
<div>
Interpret-Text is an extension of InterpretML , specifically for several text models. Three modules are provided: ClassicalTextExplainer, UnifiedInformationExplainer and IntrospectiveRationaleExplainer.
Classical Text Explainer The ClassicalTextExplainer supports linear models from sklearn with a coefs_ call and tree-based models for which feature_importances_ is defined.
ClassicalTextExplainer includes a NLP pipeline from preprocessing to hyperparameter tuning, so it accepts raw text data as input. The default pipeline uses a unigram bag-of-words model. Elements of the pipeline can be replaced if desired.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/interpret-text/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/interpretml/>InterpretML</a></h1>
<div>
The InterpretML toolkit, developed at Microsoft, can be decomposed in two major components:
A set of interpretable &ldquo;glassbox&rdquo; models Techniques for explaining black box systems. W.r.t. 1, InterpretML particularly contains a new interpretable &ldquo;glassbox&rdquo; model that combines Generalized Additive Models (GAMs) with machine learning techniques such as gradient boosted trees, called an Explainable Boosting Machine.
Other than this new interpretable model, the main utility of InterpretML is to unify existing explainability techniques under a single API.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/interpretml/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/lime/>LIME: Local Interpretable Model-agnostic Explanations</a></h1>
<div>
The type of explanation LIME offers is a surrogate model that approximates a black box prediction locally. The surrogate model is a sparse linear model, which means that the surrogate model is interpretable (in this case, it&rsquo;s weights are meaningful). This simpler model can thus help to explain the black box prediction, assuming the local approximation is actually sufficiently representative.
The intuition behind this is provided in the README:
Intuitively, an explanation is a local linear approximation of the model&rsquo;s behaviour.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/lime/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/model-card/>Model cards for Model Reporting</a></h1>
<div>
Model cards are an extension of the datasheet to machine learning models.
Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/model-card/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/openmined/>OpenMined (PySyft)</a></h1>
<div>
The OpenMined community is a collaboration of several organizations, including TensorFlow, PyTorch and Keras, to create an open-source ecosystem of privacy tools that extend libraries such as PyTorch with cryptographic techniques and differential privacy. The aim is to contribute to the adaptation of privacy-preserving AI.
To this end, OpenMined offers several privacy-preserving tools on their github. A main tool is PySyft, which allows &ldquo;computing on data you do not own and cannot see&rdquo;.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/openmined/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/shap/>SHAP: SHapley Additive exPlanations</a></h1>
<div>
The SHAP package is built on the concept of a Shapley value and can generate explanations model-agnostically. So it only requires input and output values, not model internals:
SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. (README)
Additionally, this package also contains several model-specific implementations of Shapley values that are optimized for a particular machine learning model and sometimes even for a particular library.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/shap/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/smactr/>SMACTR: End-to-End Framework for Internal Algorithmic Auditing</a></h1>
<div>
Introduction A major downside of external auditing is that it typically only can be done after model deployment. This paper presents a methodology for internal algorithmic auditing as an integral part of the development process, end-to-end.
Those who move fast and break things, beware:
The audit process is necessarily boring, slow, meticulous and methodical—antithetical to the typical rapid development pace for AI technology. However, it is critical to slow down as algorithms continue to be deployed in increasingly high-stakes domains.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/smactr/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/tensorflow-privacy/>TensorFlow Privacy</a></h1>
<div>
TensorFlow Privacy is a library that allows you to replace default TensorFlow optimizers with optimizers that allow training with differential privacy, i.e. they implement forms of stochastic gradient descent (SGD) with differential privacy.
Because large neural networks or other differentiable models have a very large learning capacity, it can happen that the model achieves high performance on uncommon training input by simply &ldquo;memorizing&rdquo; the training input. If the training data is sensitive, for example information about a specific user, this is undesired behavior that may leak private information.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/tensorflow-privacy/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/treeinterpreter/>TreeInterpreter</a></h1>
<div>
This library provides a separate predict() function for scikit-learn tree-based models (so also ensembles) that outputs a prediction with interpretable elements of the shape prediction = bias + feature_1_contribution + ... + feature_n_contribution.
That is, it turns these tree-based models into a white box , where we can inspect how much each feature contributes to the predicted value (in the case of regression) or how much it contributes to the estimated probability of a class (given classification).
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/treeinterpreter/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/what-if-tool/>What-If Tool</a></h1>
<div>
The What-If Tool (WIT) takes a pretrained model and then allows you to visualize the effect of changing e.g. classification thresholds or the data points themselves on performance, explainability and fairness metrics.
Many convenient functions for gaining insight in the data set are provided, such as binning on particular features, attribution values, or inference scores, computing partial dependence plots, and typical performance indicators such as a confusion matrix or ROC curve.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/what-if-tool/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/xai-toolbox/>XAI Toolbox</a></h1>
<div>
This library is a small toolbox that offers some convenience functions for quickly visualizing imbalances in the data set, computing (permutation) feature importances and metrics such as the ROC-curve. A function to balance the data is offered through basic up- or downsampling, but other than this no fairness criteria are defined.
Compared to other libraries the XAI Toolbox is very basic and currently the roadmap (which is not updated since 2019) does not include any major improvements.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/xai-toolbox/>Read more...</a>
</div>
</article>
</main>
<footer>
<p>Last modified on 21-06-2021.</p>
<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>
</body>
</html>