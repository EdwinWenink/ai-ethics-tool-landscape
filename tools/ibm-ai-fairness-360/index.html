<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel="shortcut icon" href=img/favicon.ico><title>IBM AI Fairness 360</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><div id=breadcrumbs><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ibm-ai-fairness-360/>Ibm ai fairness 360</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a></div></nav></header><main><article><h1 class=title>IBM AI Fairness 360</h1><div><ul><li id=values>Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness>fairness</a>
}<ul><li>Fairness type: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/group-fairness>group fairness</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/individual-fairness>individual fairness</a> }</li></ul></li><li id=categories>Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic>model-agnostic</a> }</li><li id=design-phase>Stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing>preprocessing</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/in-processing>in-processing</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/post-processing>Post-processing</a> }</li><li id=repository>Repository: <a href=https://github.com/Trusted-AI/AIF360 target=_blank>https://github.com/Trusted-AI/AIF360</a></li><li id=model>Tasks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/classification>classification</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/regression>regression</a> }</li><li id=model>Input data: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/tabular>tabular</a> }</li><li id=licence>Licence: Apache license</li><li id=languages>Languages: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/python>Python</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/r>R</a> }</li><li id=references>References:<ul><li><a href=https://arxiv.org/abs/1810.01943 target=_blank>Bellamny et al. - AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias</a></li></ul></li></ul></div><hr><div><p>The IBM AI Fairness 360 Toolkit contains several bias mitigation algorithms that are applicable to various stages of the machine learning pipeline.
The toolkit implements different notions of <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness>fairness</a>, both on <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/individual-fairness>individual</a> and the <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/group-fairness>group</a> level, and several fairness metrics for both classes of fairness.
The toolkit provides additional <a href=http://aif360.mybluemix.net/resources#guidance>guidance on choosing metrics and mitigation algorithms</a> given a particular goal and application.</p><p>The following should be noted when using the fairness toolkit (and other similar toolkits, for that matter):</p><blockquote><p>The toolkit should only be used in a very limited setting: allocation or risk assessment problems with well-defined protected attributes in which one would like to have some sort of statistical or mathematical notion of sameness. Even then, the code and collateral contained in AIF360 is only a starting point to a broader discussion among multiple stakeholders on overall decision making workflows. <a href=http://aif360.mybluemix.net/resources#guidance>source</a></p></blockquote><p>Moreover, the choice for a particular algorithm from the toolkit also depends on assumptions on the equality of people.
Within the group fairness approach, the toolkit distinguishes two different &ldquo;worldviews&rdquo; underlying group fairness: the &ldquo;we&rsquo;re all equal&rdquo; worldview and the &ldquo;what you see if what you get worldview&rdquo;. See <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/group-fairness>group fairness</a> for a further explanation.</p><p>To choose an approach, we need to consider 1) the type of fairness to strive for 2) given that type, which fairness metrics/constraints to use (if applicable) 3) where in the pipeline to intervene 4) which algorithm is suitable to support the made choices.</p><h2 id=choosing-fairness-metrics>Choosing fairness metrics</h2><p>The amount of implemented fairness metrics is very large, so we refer here to the <a href=https://aif360.readthedocs.io/en/latest/modules/metrics.html>API</a> and summarize the directions given in the guidance material.</p><h3 id=individual-vs-group>Individual vs. group</h3><p>For individual fairness, refer to the <a href=https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.SampleDistortionMetric.html#aif360.metrics.SampleDistortionMetric>SampleDistortionMetric</a>.</p><p>For group fairness, refer to <a href=https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.DatasetMetric.html#aif360.metrics.DatasetMetric>DatasetMetric</a> and its children classes <a href=https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.BinaryLabelDatasetMetric.html#aif360.metrics.BinaryLabelDatasetMetric>BinaryLabelDatasetMetric</a> and <a href=https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html#aif360.metrics.ClassificationMetric>ClassificationMetric</a>.</p><p>It is possible to combine individual- and group fairness in a single metric.
The <a href=https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html#aif360.metrics.ClassificationMetric>ClassificationMetric</a> contains several measures related to the <a href=https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html#aif360.metrics.ClassificationMetric.generalized_entropy_index>generalized entropy index</a> suitable for this purpose.</p><h3 id=stages>Stages</h3><p>The metrics under <code>(BinaryLabel)DatasetMetric</code> apply to the training data and are thus relevant for the
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing/>preprocessing</a>
stage.</p><p>The metrics under <code>ClassificationMetric</code> apply to the models themselves and are thus relevant during the
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/in-processing/>in-processing</a>
stage.</p><h3 id=worldviews>Worldviews</h3><p>In the case of group fairness, the underlying &ldquo;worldview&rdquo; can be a relevant factor in choosing metrics.</p><p>For &ldquo;we are all equal&rdquo; worldview the <em>demographic parity metrics</em> are appropriate, e.g.</p><ul><li><code>disparate_impact</code></li><li><code>statistical_parity_difference</code></li></ul><p>If the application follows the &ldquo;what you see is what you get&rdquo; worldview, then variations of the <em>equality of odds</em> fairness metric is appropriate:</p><ul><li><code>average_odds_difference</code></li><li><code>average_abs_odds_difference</code></li></ul><p>Some metrics are not specific to a particular worldview, such as metrics based on error rates, e.g.:</p><ul><li><code>false_negative_rate_ratio</code></li><li><code>false_positive_rate_ratio</code></li><li><code>error_rate_ratio</code></li></ul><h2 id=choosing-algorithms>Choosing algorithms</h2><p>The API nicely lists algorithms by the stage they are applicable in.
The algorithms that affect the
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/in-processing/>in-processing</a>
stage are mostly suitable for classification.
However, a reduction-based approach such as <a href=https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.inprocessing.GridSearchReduction.html#aif360.algorithms.inprocessing.GridSearchReduction>GridSearchReduction</a> can also be used for regression.</p><h3 id=preprocessing>Preprocessing</h3><p>For
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing/>preprocessing</a>
algorithms see <a href=https://aif360.readthedocs.io/en/latest/modules/algorithms.html#module-aif360.algorithms.preprocessing>aif360.algorithms.preprocessing</a>:</p><p>Relevant remarks from the guidance material on the preprocessing algorithms:</p><ul><li>&ldquo;Among pre-processing algorithms, reweighing only changes weights applied to training samples; it does not change any feature or label values. Therefore, it may be a preferred option in case the application does not allow for value changes.&rdquo;</li><li>&ldquo;Disparate impact remover and optimized pre-processing yield modified datasets in the same space as the input training data, whereas LFRâ€™s pre-processed dataset is in a latent space.&rdquo;</li><li>&ldquo;If the application requires transparency on the transformation, then disparate impact remover and optimized pre-processing may be preferred options&rdquo;</li><li>&ldquo;Moreover, optimized pre-processing addresses both group fairness and individual fairness.&rdquo;</li></ul><h3 id=in-processing>In-processing</h3><p>For algorithms for fair learning see <a href=https://aif360.readthedocs.io/en/latest/modules/algorithms.html#module-aif360.algorithms.inprocessing>aif360.algorithms.inprocessing</a>.</p><p>Relevant remarks from the guidance material on the &ldquo;in-processing&rdquo; algorithms:</p><ul><li>&ldquo;Among in-processing algorithms, the prejudice remover is limited to learning algorithms that allow for regularization terms whereas the adversarial debiasing algorithm allows for a more general set of learning algorithms, and may be preferred for that reason.&rdquo;</li></ul><h3 id=post-processing>Post-processing</h3><p>For
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/post-processing/>post-processing</a>
algorithms, see <a href=https://aif360.readthedocs.io/en/latest/modules/algorithms.html#module-aif360.algorithms.postprocessing>aif360.algorithms.postprocessing</a>.</p><p>Relevant remarks from the guidance material on the &ldquo;post-processing&rdquo; algorithms:</p><ul><li>&ldquo;Among post-processing algorithms, the two equalized odds post-processing algorithms have a randomized component whereas the reject option algorithm is deterministic, and may be preferred for that reason.&rdquo;</li></ul></div></article></main><footer><p>Last modified on 13-07-2021.</p><p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>