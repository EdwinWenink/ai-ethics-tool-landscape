<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>LIME: Local Interpretable Model-agnostic Explanations</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><div id=breadcrumbs><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/lime/>Lime</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a></div></nav></header><main><article><h1 class=title>LIME: Local Interpretable Model-agnostic Explanations</h1><div><ul><li id=values>Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/explainability>explainability</a>
}<ul><li>Explanation type: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/local-surrogate>local surrogate</a> }</li></ul></li><li id=categories>Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic>model-agnostic</a> }</li><li id=design-phase>Stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/post-hoc>post-hoc</a> }</li><li id=repository>Repository: <a href=https://github.com/marcotcr/lime target=_blank>https://github.com/marcotcr/lime</a></li><li id=model>Tasks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/classification>classification</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/regression>regression</a> }</li><li id=model>Input data: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/tabular>tabular</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/text>text</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/image>image</a> }</li><li id=licence>Licence: BSD 2-Clause "Simplified"</li><li id=languages>Languages: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/python>Python</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/r>R</a> }</li><li id=references>References:<ul><li><a href=https://arxiv.org/abs/1602.04938 target=_blank>Ribeiro et al. - "Why Should I Trust You?": Explaining the Predictions of Any Classifier</a></li></ul></li></ul></div><hr><div><p>The type of explanation LIME offers is a surrogate model that approximates a black box prediction <em>locally</em>.
The surrogate model is a sparse linear model, which means that the surrogate model is interpretable (in this case, it&rsquo;s weights are meaningful).
This simpler model can thus help to explain the black box prediction, assuming the local approximation is actually sufficiently representative.</p><p>The intuition behind this is provided in the README:</p><blockquote><p>Intuitively, an explanation is a local linear approximation of the model&rsquo;s behaviour. While the model may be very complex globally, it is easier to approximate it around the vicinity of a particular instance. While treating the model as a black box, we perturb the instance we want to explain and learn a sparse linear model around it, as an explanation. The figure below illustrates the intuition for this procedure. The model&rsquo;s decision function is represented by the blue/pink background, and is clearly nonlinear. The bright red cross is the instance being explained (let&rsquo;s call it X). We sample instances around X, and weight them according to their proximity to X (weight here is indicated by size). We then learn a linear model (dashed line) that approximates the model well in the vicinity of X, but not necessarily globally. (README)</p></blockquote><p><img src=https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/lime.png alt="Visualization of local linear approximation"></p><p>Having a global approximation of a black box classifier would mean that we can have a white box classifier that is able to accurately approximate the black box predictions for <em>any</em> input.
If we would have such a white box model, we would have probably used that model instead of the black box.
<em>Locally</em>, i.e. in the more limited feature range of a <em>single</em> input, white box classifiers can more reliably approximate the black box predictions.
Notice that the local surrogate does not predict the true labels, but the labels as predicted by the black box model.</p><p>The authors of LIME also wrote an accessible <a href=https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/>blog post</a> explaining LIME.</p></div></article></main><footer><p>Last modified on 25-06-2021.</p><p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>