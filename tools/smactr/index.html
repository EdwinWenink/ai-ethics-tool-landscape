<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>SMACTR: End-to-End Framework for Internal Algorithmic Auditing</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><div id=breadcrumbs><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/smactr/>Smactr</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a></div></nav></header><main><article><h1 class=title>SMACTR: End-to-End Framework for Internal Algorithmic Auditing</h1><div><ul><li id=values>Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/accountability>accountability</a>
}</li><li id=categories>Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic>model-agnostic</a> }</li><li id=design-phase>Stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/design-phase>design phase</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing>preprocessing</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/in-processing>in-processing</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/post-processing>post-processing</a> }</li><li id=references>References:<ul><li><a href=https://arxiv.org/abs/2001.00973 target=_blank>Raji et al. - Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing</a></li></ul></li></ul></div><hr><div><h2 id=introduction>Introduction</h2><p>A major downside of external auditing is that it typically only can be done <em>after</em> model deployment.
This paper presents a methodology for <em>internal</em> algorithmic auditing as an integral part of the development process, end-to-end.</p><p>Those who move fast and break things, beware:</p><blockquote><p>The audit process is necessarily boring, slow, meticulous and methodical—antithetical to the typical rapid development pace for AI technology. However, it is critical to slow down as algorithms continue to be deployed in increasingly high-stakes domains. (33)</p></blockquote><h2 id=five-stages-of-smactr>Five stages of SMACTR</h2><ol><li>Scoping</li><li>Mapping</li><li>Artefact Collection</li><li>Testing</li><li>Reflection</li></ol><p>For each stage the SMACTR method recommends a certain type of key artefact as output.
These stages are described in detail in the paper, but a short summary is provided below.
What&rsquo;s quite nice about this paper is that it comes with <a href=https://drive.google.com/drive/folders/1GWlq8qGZXb2lNHxWBuo2wl-rlHsjNPM0>supplementary materials</a> offering templates for some key artefacts.
Some artefacts are very project-specific and do not have a general template.</p><h3 id=scoping>Scoping</h3><blockquote><p>The goal of the scoping stage is to clarify the objective of the audit by reviewing the motivations and intended impact of the investigated system, and confirming the principles and values meant to guide product development. This is the stage in which the risk analysis begins by mapping out intended use cases and identifying analogous deployments either within the organization or from competitors or adjacent industries. The goal is to anticipate areas to investigate as potential sources of harm and social impact. At this stage, interaction with the system should be minimal. (39)</p></blockquote><p>Pre-requisite documents:</p><ul><li>Declaration or confirmation statement of the ethical principles, standards and AI principles.</li><li>Product Requirements Document (PRD) or something similar, like a project planning</li></ul><p><a href=https://drive.google.com/drive/folders/1-NR9dumIy5tiAMDTaZVkdXKkYraCStJX>Key artefacts of auditers</a>:</p><ol><li>Ethical review of system use case</li><li>Social Impact Assessment</li></ol><h3 id=mapping>Mapping</h3><p>This is not yet a testing phase, but is about relating the documents from the scoping phase to those stakeholders that will be involved in the actual testing/reviewing.</p><ul><li>map internal stakeholders<ul><li>&ldquo;enables an internal record of individual accountability with respect to participation towards the final outcome&rdquo; (39)</li><li>&ldquo;enables the trace of relevant contacts for future inquiry&rdquo; (39)</li></ul></li><li>identify key collaborators for the internal audit</li><li>&ldquo;orchestrate the appropriate stakeholder buy-in required for execution&rdquo; (39)</li><li>initiate risk prioritization for later testing<ul><li>identify failure modes, e.g. what&rsquo;s the impact of false negatives and false positives?</li></ul></li></ul><p><a href=https://drive.google.com/drive/folders/1KuOCwhrUrlrLAhkhbNMzXFFOXG5Mgq7_>Key artefacts</a>:</p><ol><li>Stakeholder map and collaborator contact list</li><li>System map of product development lifecycle and engineering system overview, especially when multiple models inform the end product</li><li>Design history file review of past product versions</li><li>&ldquo;Report or interview transcripts on key findings from internal ethnographic fieldwork involving the stakeholders and engineers.&rdquo; (40)</li></ol><h3 id=artifact-collection>Artifact Collection</h3><blockquote><p>Often this implies a record of data and model dynamics though application-based systems can include other product development artifacts such as design documents and reviews, in addition to systems architecture diagrams and other
implementation planning documents and retrospectives.</p></blockquote><p><a href=https://drive.google.com/drive/folders/1l8RkF5FBIM1sVoL9FKep9nmJpM-mXs86>Key artefacts</a>:</p><ul><li>Audit checklist that is used to verify that all documentation required for the audit is provided<ul><li>Can include model and data transparency documentation</li></ul></li><li>Datasheets and model cards</li></ul><h3 id=testing>Testing</h3><p>The testing phase can be highly dependent on the specific usage context and the project.
The paper discusses two <a href=https://drive.google.com/drive/folders/1vMEBzexh8_3-0g2TxTBU8gejbaKHz8gq>example artifacts</a>:</p><ul><li>Report on adversial testing</li><li>Ethical risk analysis chart</li></ul><p>The goal of this phase is to test compliance with <em>prioritized</em> ethical values.
Such a prioritization can be based on a risk assessment methodology such as a so-called &ldquo;failure modes and effects analysis&rdquo; (FMEA), which is very common in aerospace engineering:</p><blockquote><p>The main purpose of a FMEA is to define, identify and eliminate potential failures or problems in different products, designs, systems and services. Prior to conducting a FMEA, known issues with a proposed technology should be thoroughly mapped through a literature review and by collecting and documenting the experiences of the product designers, engineers and managers. Further, the risk exercise is based on known issues with relevant datasets and models, information that can be gathered from interviews and from extant technical documentation. (36)</p></blockquote><h3 id=reflection>Reflection</h3><ul><li>Discuss results in relation to the original scoping.</li><li>Based on test results, highlight the ethical principles that may be jeopardized when the AI system is deployed</li></ul><p><a href=https://drive.google.com/drive/folders/1IHwiJwSjz1UjJ-lw5wNOkG-Q73LbxwKp>Key artefacts</a>:</p><ul><li>Algorithmic Use-related Risk Analysis<ul><li>The above mentioned FMEA can be the basis for this</li></ul></li><li>Risk mitigation plan to counter identified failures; joint effort by auditers and engineers<ul><li>E.g. after identifying class imbalance issues related to fairness, decide what is acceptable in terms of class parity and then include more data for balancing</li><li>E.g. make design choices like opt-out or opt-in functionality</li></ul></li><li>Algorithmic Design History File (ADHF)<ul><li>&ldquo;collect all the documentation from the activities outlined above related to the development of the algorithm. It should point to the documents necessary to demonstrate that the product or model was developed in accordance with an organization’s ethical values, and that the benefits of the product outweigh any risks identified in the risk analysis process.&rdquo; (42)</li></ul></li><li>Algorithmic Audit Summary Report<ul><li>&ldquo;The report aggregates all key audit artifacts, technical analyses and documentation, putting this in one accessible location for review. This audit report should be compared qualitatively and quantitatively to the expectations outlined in the given ethical objectives and any corresponding engineering requirements.&rdquo; (42)</li></ul></li></ul></div></article></main><footer><p>Last modified on 13-07-2021.</p><p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>