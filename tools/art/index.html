<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<link rel="shortcut icon" href=img/favicon.ico>
<title>ART: Adversial Robustness 360 Toolbox</title>
<link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css>
</head>
<body>
<header>
<div class=inline-block>
<a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a>
</div>
<nav>
<div id=breadcrumbs>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/art/>Art</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a>
</div>
</nav>
</header>
<main>
<article>
<h1 class=title>ART: Adversial Robustness 360 Toolbox</h1>
<div>
<ul>
<li id=values> Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security>security</a>
}
</li>
<li id=categories> Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-specific>model-specific</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic>model-agnostic</a> } </li>
<li id=design-phase> Stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing>preprocessing</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/in-processing>in-processing</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/post-processing>post-processing</a> }</li>
<li id=repository> Repository: <a href=https://github.com/Trusted-AI/adversarial-robustness-toolbox target=_blank>https://github.com/Trusted-AI/adversarial-robustness-toolbox</a></li>
<li id=model>Tasks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/classification>classification</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/object-detection>object detection</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/speech-recognition>speech recognition</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/generation>generation</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/certification>certification</a> }</li>
<li id=model>Input data: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/tabular>tabular</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/text>text</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/image>image</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/audio>audio</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/video>video</a> }</li>
<li id=licence> Licence: MIT</li>
<li id=languages> Languages: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/python>Python</a> }</li>
<li id=references>References:
<ul>
<li><a href=https://arxiv.org/abs/1807.01069 target=_blank>Nicolae et al. - Adversarial Robustness Toolbox v1.0.0</a></li>
</ul>
</li>
</ul>
</div>
<hr>
<div>
<p>The Adversial Robustness Toolbox (ART) is the first comprehensive toolbox that unifies many defensive techniques for four categories of adversarial attacks on machine learning models.
These categories are model evasion, model poisoning, model extraction and inference (e.g. inference of sensitive attributes in the training data; or determining whether an example was part of the training data).
ART supports all popular machine learning frameworks, all data types and all machine learning tasks.</p>
<p><img src=https://raw.githubusercontent.com/Trusted-AI/adversarial-robustness-toolbox/main/docs/images/adversarial_threats_attacker.png alt="Adversial machine learning attacks"></p>
<p>The documentation of ART is excellent because a whole wiki is hosted on the github repository.
On the github repostory you can find an extensive explanation of the different categories of <a href=https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks>attacks</a> with paper references for many concrete attacks.
Many of these attacks are also implemented in ART.
An overview of <a href=https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Defences>defences</a> is also provided.
Some attacks assume white box models, whereas others assume black box models.
The toolbox addresses both model-specific and model-agnostic techniques.</p>
<p>Additionally, ART provides tools (such as metrics) to <a href=https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Metrics>evaluate</a> the robustness of machine learning methods and applications.</p>
<p>With all techniques combined, ART provides offensive and defensive techniques for each stage of the machine learning pipeline.</p>
</div>
</article>
</main>
<footer>
<p>Last modified on 22-07-2021.</p>
<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>
</body>
</html>