<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel="shortcut icon" href=img/favicon.ico><title>What-If Tool</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><div id=breadcrumbs><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/what-if-tool/>What if tool</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a></div></nav></header><main><article><h1 class=title>What-If Tool</h1><div><ul><li id=values>Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness>fairness</a>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/explainability>explainability</a>
}<ul><li>Explanation type: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley>Shapley</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/gradient-based>gradient-based</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual>counterfactual</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/partial-dependence-plot>partial dependence plot</a> }</li></ul><ul><li>Fairness type: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/individual-fairness>individual fairness</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/group-fairness>group fairness</a> }</li></ul></li><li id=categories>Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic>model-agnostic</a> }</li><li id=design-phase>Stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/post-processing>post-processing</a> }</li><li id=repository>Repository: <a href=https://github.com/pair-code/what-if-tool target=_blank>https://github.com/pair-code/what-if-tool</a></li><li id=model>Tasks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/classification>classification</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/regression>regression</a> }</li><li id=model>Input data: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/tabular>tabular</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/image>image</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/text>text</a> }</li><li id=licence>Licence: Apache</li><li id=languages>Languages: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/python>Python</a> }</li><li id=references>References:<ul><li><a href=https://pair-code.github.io/what-if-tool/ai-fairness.html target=_blank>Explanation of supported fairness metrics</a></li></ul><ul><li><a href=https://pair-code.github.io/what-if-tool/index.html#demos target=_blank>Demo site</a></li></ul></li></ul></div><hr><div><p>The What-If Tool (WIT) takes a pretrained model and then allows you to visualize the effect of changing e.g. classification thresholds or the data points themselves on performance, explainability and fairness metrics.</p><p>Many convenient functions for gaining insight in the data set are provided, such as binning on particular features, attribution values, or inference scores, computing partial dependence plots, and typical performance indicators such as a confusion matrix or ROC curve.</p><p>Because you can interactively edit the data set or prediction thresholds, you can also answer &ldquo;what if&rdquo; questions, which goes beyond typical interactive visualization.
For example, what happens to the demographic parity fairness metric when I adjust the classification threshold?
And how do the Shapley value for the features change?</p><p>The What-If Tool works with python-accessible models in a notebook, and works with most models hosted by TF-serving in Tensorboard.</p><h2 id=explainability>Explainability</h2><p>In terms of explainability, WIT hooks into existing feature attribution methods like
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/shap/>SHAP</a>
and
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/lime/>LIME</a>
when used in notebook mode.</p><p>The following <a href="https://colab.research.google.com/github/PAIR-code/what-if-tool/blob/master/WIT_COMPAS_with_SHAP.ipynb#scrollTo=KF00pJvkeicT">demo</a> shows how to use Shapley values in WIT.
As the title of the tool already suggests, WIT also computes the
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/>counterfactual</a>
for any data point given some distance metric (potentially a custom metric).</p><h2 id=fairness>Fairness</h2><p>Supported fairness metrics:</p><ul><li>Group unaware / individual fairness</li><li>Group threshold (adjust classification threshold per group to compensate for historical bias against that group)</li><li>Demographic parity</li><li>Equal opportunity</li><li>Equal accuracy</li></ul><p>For binary classification models you can use fairness optimization strategies.</p></div></article></main><footer><p>Last modified on 16-07-2021.</p><p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>