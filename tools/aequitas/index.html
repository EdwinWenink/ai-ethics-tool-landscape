<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Aequitas: Bias and Fairness Audit Toolkit</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/>Values</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/>Stages</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/>Categories</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/taxonomy/>Taxonomy</a></nav></header><main><article><h1 class=title>Aequitas: Bias and Fairness Audit Toolkit</h1><div><ul><li id=values>Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness>fairness</a>
}<ul><li>Fairness type: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/group-fairness>group fairness</a> }</li></ul></li><li id=categories>Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-agnostic>model-agnostic</a> }</li><li id=design-phase>Design stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/preprocessing>preprocessing</a> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/post-hoc>post-hoc</a> }</li><li id=repository>Repository: <a href=https://github.com/dssg/aequitas target=_blank>https://github.com/dssg/aequitas</a></li><li id=model>Tasks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/classification>classification</a> }</li><li id=model>Input data: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/tabular>tabular</a> }</li><li id=licence>Licence: MIT</li><li id=languages>Languages: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/python>Python</a> }</li><li id=references>References:<ul><li><a href=http://www.datasciencepublicpolicy.org/projects/aequitas/>Aequitas project page</a></li></ul><ul><li><a href=http://aequitas.dssg.io/>Aequitas: Bias and Fairness Audit Toolkit</a></li></ul><ul><li><a href=https://arxiv.org/abs/1811.05577>Saleiro et al. - Aequitas: A Bias and Fairness Audit Toolkit</a></li></ul></li></ul></div><hr><div><h2 id=audit>Audit</h2><p>The Aequitas toolkit can both be used on the command-line, programmatically via its <a href=https://github.com/dssg/aequitas#input-data-for-python-api>Python API</a> or via a web interface.
The <a href=http://aequitas.dssg.io/>web interface</a> offers a four step programme to audit a dataset on bias.
The four steps are:</p><ol><li>Upload (tabular) data</li><li>Determine protected groups and reference group</li><li>Select fairness metrics and disparity intolerance</li><li>Inspect bias report<ul><li><a href=http://aequitas.dssg.io/example.html>Example audit report</a>.</li></ul></li></ol><p>This toolkit is useful for auditing bias and fairness according to a limited set of common fairness metrics, but does not offer algorithms for mitigating bias.
It thus only requires input data and model outputs and does not intervene in the training of the model itself.</p><p><a href=https://dssg.github.io/aequitas/>Further documentation</a>.</p><h2 id=fairness-metrics>Fairness metrics</h2><p>The amount of supported fairness metrics is relatively limited compared to other fairness toolkits.
They are about representatitivess and error rates per group, in this case relative to a reference group.</p><p>The following disparities are computed in the web interface:</p><ul><li>Equal Parity</li><li>Proportional Parity</li><li>False Positive Rate Parity</li><li>False Discovery Rate Parity</li><li>False Negative Rate Parity</li><li>False Omission Rate Parity</li></ul><p>With the Python API you can compute a few closely interrelated disparaties.</p><h2 id=fairness-tree>Fairness tree</h2><p>What&rsquo;s particularly nice about this toolkit is the provided
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairness-tree/>fairness tree</a>
, which can also be very helpful in combination with other toolkits.</p><p><img src=http://www.datasciencepublicpolicy.org/wp-content/uploads/2021/04/Fairness-Full-Tree-1200x908.png alt="Complete Fairness Tree"></p><p><img src=http://www.datasciencepublicpolicy.org/wp-content/uploads/2021/04/Fairness-Short-Tree-1200x746.png alt="Zoomed in Fairness Tree"></p></div></article></main><footer>Last modified on 28-05-2021.<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>