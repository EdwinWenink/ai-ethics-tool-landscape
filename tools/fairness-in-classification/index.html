<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel="shortcut icon" href=img/favicon.ico><title>Fairness in Classification</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><div id=breadcrumbs><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools>Tools</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/fairness-in-classification/>Fairness in classification</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a></div></nav></header><main><article><h1 class=title>Fairness in Classification</h1><div><ul><li id=values>Values: {
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/fairness>fairness</a>
}<ul><li>Fairness type: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/fairness/group-fairness>group fairness</a> }</li></ul></li><li id=categories>Categories: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/categories/model-specific>model-specific</a> }</li><li id=design-phase>Stage: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/in-processing>in-processing</a> }</li><li id=repository>Repository: <a href=https://github.com/mbilalzafar/fair-classification target=_blank>https://github.com/mbilalzafar/fair-classification</a></li><li id=model>Tasks: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tasks/classification>classification</a> }</li><li id=model>Input data: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/data/tabular>tabular</a> }</li><li id=licence>Licence: GNU General Public License v3</li><li id=languages>Languages: { <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/languages/python>Python</a> }</li><li id=references>References:<ul><li><a href=https://arxiv.org/abs/1507.05259 target=_blank>Muhammad Bilal Zafar et al. - Fairness Constraints: Mechanisms for Fair Classification (2017a)</a></li></ul><ul><li><a href=https://arxiv.org/abs/1610.08452 target=_blank>Muhammad Bilal Zafar et al. - Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment (2017b)</a></li></ul><ul><li><a href=https://arxiv.org/abs/1707.00010 target=_blank>Muhammad Bilal Zafar et al. - From Parity to Preference-based Notions of Fairness in Classification (2017c)</a></li></ul></li></ul></div><hr><div><p>The not-so originally named &ldquo;fairness in classification&rdquo; provides a Python implementation of three fairness constraints for logistic regression:</p><ol><li>Disparate impact: similar acceptance rate for different demographic groups. See Zafar et al., 2017 a.</li><li>Disparate mistreatment: similar misclassification rate for different demographic groups. See Zafar et al., 2017b</li><li>Preference-based fairness (as opposed to parity-based fairness): a more game-theoretical approach where decision boundaries are chosen such that it can be shown that each group prefers its own decision boundary, if rational. See Zafar et al., 2017c.</li></ol><p>This library is a good demo on how to implement fairness constraints from scratch, more than being a comprehensive fairness toolkit.</p></div></article></main><footer><p>Last modified on 19-07-2021.</p><p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>