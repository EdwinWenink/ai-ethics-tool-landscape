<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Shapley on Ethical AI Tool Landscape</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley/</link><description>Recent content in Shapley on Ethical AI Tool Landscape</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Edwin Wenink</copyright><atom:link href="https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley/index.xml" rel="self" type="application/rss+xml"/><item><title>What-If Tool</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/what-if-tool/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/what-if-tool/</guid><description>The What-If Tool (WIT) takes a pretrained model and then allows you to visualize the effect of changing e.g. classification thresholds or the data points themselves on performance, explainability and fairness metrics.
Many convenient functions for gaining insight in the data set are provided, such as binning on particular features, attribution values, or inference scores, computing partial dependence plots, and typical performance indicators such as a confusion matrix or ROC curve.</description></item></channel></rss>