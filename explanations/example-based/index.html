<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<link rel="shortcut icon" href=img/favicon.ico>
<title>Example-based explanations</title>
<link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css>
<link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/example-based/index.xml title="Ethical AI Tool Landscape">
</head>
<body>
<header>
<div class=inline-block>
<a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a>
</div>
<nav>
<div id=breadcrumbs>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations>Explanations</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/example-based/>Example based</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a>
</div>
</nav>
</header>
<main>
<div>
<h1 class=title>Example-based explanations</h1>
<div><p>Example-based explanations can be used either to provide more insight into a data set or insight into the predictions of machine learning models.
The explanations themselves are (a selection of) data instances, which assumes that data instances can be presented in a meaningful way.
For example, a data instance with 1000 features is not going to provide meaningful insights for humans.</p>
<p>Insight into data can be gained by providing <em>prototypical</em> examples, i.e. data instances that are representative of the data.
Model predictions can in turn be explained by referring to similar examples for which the same prediction is made.</p>
<p>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/>Counterfactual</a>
and
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/contrastive/>contrastive</a>
examples are special cases for which this taxonomy has separate entries.
Adversarial examples are discussed under
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/security/>security</a>
.</p>
</div>
</div>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ai-explainability-360/>AI Explainability 360</a></h1>
<div>
The AI Explainability 360 (AIX360) toolkit is a Python library that offers a wide range of explanation types as well as some explainability metrics. AIX360 offers excellent guidance material, an interactive demo as well as developer tutorials. What&rsquo;s particularly good about this material is that it stimulates reflection on which type of explanation is appropriate, not only from a technical point of view, but also with respect to the target explainer and explainee.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ai-explainability-360/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Alibi</a></h1>
<div>
Alibi is an open-source Python library that supports various interpretability techniques and a broad array of explanation types. The README already provides an overview of the supported methods and when they are applicable. The following table with supported methods is copied from the README (slightly abbreviated):
Supported methods Method Models Explanations Classification Regression Tabular Text Images Categorical features ALE BB global ✔ ✔ ✔ Anchors BB local ✔ ✔ ✔ ✔ ✔ CEM BB* TF/Keras local ✔ ✔ ✔ Counterfactuals BB* TF/Keras local ✔ ✔ ✔ Prototype Counterfactuals BB* TF/Keras local ✔ ✔ ✔ ✔ Integrated Gradients TF/Keras local ✔ ✔ ✔ ✔ ✔ ✔ Kernel SHAP BB local global ✔ ✔ ✔ ✔ Tree SHAP WB local global ✔ ✔ ✔ ✔ The README also explains the keys:
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/dice/>DiCE: Diverse Counterfactual Explanations</a></h1>
<div>
From README:
DiCE implements counterfactual (CF) explanations that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., you would have received the loan if your income was higher by $10,000. In other words, it provides &ldquo;what-if&rdquo; explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.
A main innovation of DiCE is that it implements a method to make producing counter-factual examples more model-agnostic:
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/dice/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/interpret-text/>Interpret-Text</a></h1>
<div>
Interpret-Text is an extension of InterpretML , specifically for several text models. Three modules are provided: ClassicalTextExplainer, UnifiedInformationExplainer and IntrospectiveRationaleExplainer.
Classical Text Explainer The ClassicalTextExplainer supports linear models from sklearn with a coefs_ call and tree-based models for which feature_importances_ is defined.
ClassicalTextExplainer includes a NLP pipeline from preprocessing to hyperparameter tuning, so it accepts raw text data as input. The default pipeline uses a unigram bag-of-words model. Elements of the pipeline can be replaced if desired.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/interpret-text/>Read more...</a>
</div>
</article>
</main>
<footer>
<p>Last modified on 02-08-2021.</p>
<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>
</body>
</html>