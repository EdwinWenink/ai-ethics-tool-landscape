<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>White box explanations</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css><link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/white-box/index.xml title="Ethical AI Tool Landscape"></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><div id=breadcrumbs><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations>Explanations</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/white-box/>White box</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a></div></nav></header><main><div><h1 class=title>White box explanations</h1><div><p>Typically a distinction is made between &ldquo;black box&rdquo; models and &ldquo;white box&rdquo; models.
White box models are models where it is meaningful to look at the model&rsquo;s components.
The most simple example is a linear regression problem, where the regression coefficients can (i.e. given proper preprocessing etc.) give insights into how much each feature contributes to the final prediction.</p><p>Some explainability packages do not try to explain &ldquo;black boxes&rdquo;, but instead propose using white box models either as a replacement for the black box model, or a surrogate to it.
For the latter case, see
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/local-surrogate/>local surrogate</a>
and
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/global-surrogate/>global surrogate</a>
.</p></div></div><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/eli5/>ELI5</a></h1><div>ELI5 provides model-specific support for models from scikit-learn, lightning, decision tree ensembles using the xgboost, LightGBM, CatBoost libraries. ELI5 mainly provides convenient wrappers to couple the feature importance coefficients that these libraries already provide with feature names, as well as convenient ways to visualize importances, e.g. by highlighting words in a text. For Keras image classifiers an implementation of the gradient-based Grad-CAM visualizations is offered, but the TensorFlow V2 backend is not supported.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/eli5/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/interpretml/>InterpretML</a></h1><div>The InterpretML toolkit, developed at Microsoft, can be decomposed in two major components:
A set of interpretable &ldquo;glassbox&rdquo; models Techniques for explaining black box systems. W.r.t. 1, InterpretML particularly contains a new interpretable &ldquo;glassbox&rdquo; model that combines Generalized Additive Models (GAMs) with machine learning techniques such as gradient boosted trees, called an Explainable Boosting Machine.
Other than this new interpretable model, the main utility of InterpretML is to unify existing explainability techniques under a single API.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/interpretml/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/shap/>SHAP: SHapley Additive exPlanations</a></h1><div>The SHAP package is built on the concept of a Shapley value and can generate explanations model-agnostically. So it only requires input and output values, not model internals:
SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. (README)
Additionally, this package also contains several model-specific implementations of Shapley values that are optimized for a particular machine learning model and sometimes even for a particular library.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/shap/>Read more...</a></div></article></main><footer><p>Last modified on 30-06-2021.</p><p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>