<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Explanations on Ethical AI Tool Landscape</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/</link><description>Recent content in Explanations on Ethical AI Tool Landscape</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Edwin Wenink</copyright><atom:link href="https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/index.xml" rel="self" type="application/rss+xml"/><item><title>Accumulated Local Effects (ALE)</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ale/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ale/</guid><description>Computes feature effects (first-order) on a model for a given dataset (tabular). ALE expresses for a given feature how, on average, it influences the prediction of a model.</description></item><item><title>Anchor</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/anchor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/anchor/</guid><description>An &amp;ldquo;anchor&amp;rdquo; is a subset of features and their value ranges for which the model will almost always output the same prediction. The anchor should be as small as possible, also to promote interpretability.</description></item><item><title>Contrastive explanations</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/contrastive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/contrastive/</guid><description>Human explanations are often contrastive, meaning that they do not answer the indeterminate &amp;ldquo;Why?&amp;rdquo; question, but instead &amp;ldquo;Why P, rather than Q?&amp;rdquo;. For example, when a mortgage application is denied, we are not interested in a very long list of tiny little details that all contributed to that decision, but we want a to-the-point explanation that shows us what we minimally have to change to get the mortgage.
For example, the CEM method supports such an explanation by finding the minimal set of features that lead to prediction P (so this looks like an anchor explanation), and additionally a minimal set of features that should be absent to maintain decision P instead of the closest class Q (which is somewhat similar to a counterfactual ).</description></item><item><title>Counterfactual explanations</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/</guid><description>A clear description counterfactual explanations, which is very important for human causal reasoning, is provided by Molnar &amp;amp; Dandl:
A counterfactual explanation describes a causal situation in the form: &amp;ldquo;If X had not occurred, Y would not have occurred&amp;rdquo;. For example: &amp;ldquo;If I hadn&amp;rsquo;t taken a sip of this hot coffee, I wouldn&amp;rsquo;t have burned my tongue&amp;rdquo;. Event Y is that I burned my tongue; cause X is that I had a hot coffee.</description></item><item><title>example-based</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/example-based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/example-based/</guid><description/></item><item><title>Global surrogate models</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/global-surrogate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/global-surrogate/</guid><description>One approach to explain predictions of &amp;ldquo;black box&amp;rdquo; models is to approximate its predictions with a white box model that is interpretable. A global surrogate model is a model that tries to do this approximation for a larger set of instances, ideally the whole input feature range. Also see local surrogate models.</description></item><item><title>Gradient-based explanations</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/gradient-based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/gradient-based/</guid><description>Gradient-based explanation methods use gradients (e.g. in deep neural networks) to evaluate the contribution of a model component on the model output. Some methods use gradients for evaluating the contribution of input features on the output, but others instead show the influence of a single (hidden) neuron on the output, or instead the influence of an input feature on a particular neuron&amp;rsquo;s activation value.
In the case of computer vision (where deep convolutional neural networks are heavily used), one can show gradient-based information as a salience map.</description></item><item><title>ICE plot</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ice-plot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ice-plot/</guid><description/></item><item><title>Local surrogate models</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/local-surrogate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/local-surrogate/</guid><description>One approach to explain predictions of &amp;ldquo;black box&amp;rdquo; models is to approximate its predictions with a white box model that is interpretable. A local surrogate model is a model that does this approximation more accurately only in the &amp;ldquo;local&amp;rdquo; feature space corresponding to a single input. The idea is that even though a white box model may not accurately capture the behavior of a black box model globally (the feature space of a large set of training instances), it may e.</description></item><item><title>partial dependence plot</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/partial-dependence-plot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/partial-dependence-plot/</guid><description/></item><item><title>Perturbation</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/perturbation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/perturbation/</guid><description>Explainability methods based on permutation explain the importance of a feature for a particular prediction by perturbating the feature and then investigating how this change affects the outcome.</description></item><item><title>sensitivity analysis</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/sensitivity-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/sensitivity-analysis/</guid><description/></item><item><title>Shapley value explanations</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley-value/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley-value/</guid><description>Methods using Shapley values are very influential and use a particular form of perturbation to explain the importance of features for predictions. The importance of a feature for a prediction is measured in terms of how much this feature &amp;ldquo;pushes&amp;rdquo; the prediction away from a relevant baseline value, such as an average over a reference set. Different methods use different forms of perturbation. One could for example look at the difference in model output between 1) when a feature is present and 2) when a feature is occluded, repeat this for all subsets of features, and then compute the average difference per feature.</description></item><item><title>White box explanations</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/white-box/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/white-box/</guid><description>Typically a distinction is made between &amp;ldquo;black box&amp;rdquo; models and &amp;ldquo;white box&amp;rdquo; models. White box models are models where it is meaningful to look at the model&amp;rsquo;s components. The most simple example is a linear regression problem, where the regression coefficients can (i.e. given proper preprocessing etc.) give insights into how much each feature contributes to the final prediction.
Some explainability packages do not try to explain &amp;ldquo;black boxes&amp;rdquo;, but instead propose using white box models either as a replacement for the black box model, or a surrogate to it.</description></item></channel></rss>