<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Explanations on Ethical AI Tool Landscape</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/</link><description>Recent content in Explanations on Ethical AI Tool Landscape</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Edwin Wenink</copyright><atom:link href="https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/index.xml" rel="self" type="application/rss+xml"/><item><title>Accumulated Local Effects (ALE)</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ale/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ale/</guid><description>Computes feature effects (first-order) on a model for a given dataset (tabular). ALE expresses for a given feature how, on average, it influences the prediction of a model.</description></item><item><title>Anchor</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/anchor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/anchor/</guid><description>An &amp;ldquo;anchor&amp;rdquo; is a subset of features and their value ranges for which the model will almost always output the same prediction. The anchor should be as small as possible, also to promote interpretability.</description></item><item><title>Contrastive explanations</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/contrastive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/contrastive/</guid><description>Human explanations are often contrastive, meaning that they do not answer the indeterminate &amp;ldquo;Why?&amp;rdquo; question, but instead &amp;ldquo;Why P, rather than Q?&amp;rdquo;. For example, when a mortgage application is denied, we are not interested in a very long list of tiny little details that all contributed to that decision, but we want a to-the-point explanation that shows us what we minimally have to change to get the mortgage.
For example, the CEM method supports such an explanation by finding the minimal set of features that lead to prediction P (so this looks like an anchor explanation), and additionally a minimal set of features that should be absent to maintain decision P instead of the closest class Q (which is somewhat similar to a counterfactual ).</description></item><item><title>counterfactual</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/</guid><description/></item><item><title>example-based</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/example-based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/example-based/</guid><description/></item><item><title>global surrogate</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/global-surrogate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/global-surrogate/</guid><description/></item><item><title>gradient-based</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/gradient-based/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/gradient-based/</guid><description/></item><item><title>ICE plot</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ice-plot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ice-plot/</guid><description/></item><item><title>local surrogate</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/local-surrogate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/local-surrogate/</guid><description/></item><item><title>partial dependence plot</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/partial-dependence-plot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/partial-dependence-plot/</guid><description/></item><item><title>permutation</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/permutation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/permutation/</guid><description/></item><item><title>perturbation</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/perturbation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/perturbation/</guid><description/></item><item><title>sensitivity analysis</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/sensitivity-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/sensitivity-analysis/</guid><description/></item><item><title>Shapley value</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley-value/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley-value/</guid><description/></item><item><title>white box</title><link>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/white-box/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/white-box/</guid><description/></item></channel></rss>