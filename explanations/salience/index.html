<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>salience</title><link rel=stylesheet href=/css/style.css><link rel=alternate type=application/rss+xml href=/explanations/salience/index.xml title="Ethical AI Tool Landscape"></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape><img src=/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><a href=http://example.org/>Ethical AI Tool Landscape</a>
> <a href=/values/>Values</a>
> <a href=/stages/>Stages</a>
> <a href=/categories/>Categories</a>
> <a href=/tools/>Tools</a>
> <a href=/taxonomy/>Taxonomy</a></nav></header><main><div><h1 class=title>salience</h1></div><article><h1><a href=http://example.org/tools/interpret-text/>Interpret-Text</a></h1><div>Interpret-Text is an extension of InterpretML , specifically for several text models. Three modules are provided: ClassicalTextExplainer, UnifiedInformationExplainer and IntrospectiveRationaleExplainer.
Classical Text Explainer The ClassicalTextExplainer supports linear models from sklearn with a coefs_ call and tree-based models for which feature_importances_ is defined.
ClassicalTextExplainer includes a NLP pipeline from preprocessing to hyperparameter tuning, so it accepts raw text data as input. The default pipeline uses a unigram bag-of-words model. Elements of the pipeline can be replaced if desired.
<a href=http://example.org/tools/interpret-text/>Read more...</a></div></article><article><h1><a href=http://example.org/tools/shap/>SHAP: SHapley Additive exPlanations</a></h1><div>The SHAP package is built on the concept of a Shapley value and can generate explanations model-agnostically. So it only requires input and output values, not model internals:
SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. (README)
Additionally, this package also contains several model-specific implementations of Shapley values that are optimized for a particular machine learning model and sometimes even for a particular library.
<a href=http://example.org/tools/shap/>Read more...</a></div></article></main><footer>Last modified on 03-06-2021.<p>&copy; 2021 <a href=http://example.org/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>