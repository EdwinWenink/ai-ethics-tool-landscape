<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<link rel="shortcut icon" href=img/favicon.ico>
<title>Explanations</title>
<link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css>
<link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/index.xml title="Ethical AI Tool Landscape">
</head>
<body>
<header>
<div class=inline-block>
<a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a>
</div>
<nav>
<div id=breadcrumbs>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/>Explanations</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a>
</div>
</nav>
</header>
<main>
<div>
<h1 class=title>Explanations</h1>
</div>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ale/>Accumulated Local Effects (ALE)</a></h1>
<div>
Computes feature effects (first-order) on a model for a given dataset (tabular). ALE expresses for a given feature how, on average, it influences the prediction of a model.
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/anchor/>Anchor</a></h1>
<div>
An &ldquo;anchor&rdquo; is a subset of features and their value ranges for which the model will almost always output the same prediction. The anchor should be as small as possible, also to promote interpretability.
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/contrastive/>Contrastive explanations</a></h1>
<div>
Human explanations are often contrastive, meaning that they do not answer the indeterminate &ldquo;Why?&rdquo; question, but instead &ldquo;Why P, rather than Q?&rdquo;. For example, when a mortgage application is denied, we are not interested in a very long list of tiny little details that all contributed to that decision, but we want a to-the-point explanation that shows us what we minimally have to change to get the mortgage.
For example, the CEM method supports such an explanation by finding the minimal set of features that lead to prediction P (so this looks like an anchor explanation), and additionally a minimal set of features that should be absent to maintain decision P instead of the closest class Q (which is somewhat similar to a counterfactual ).
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/contrastive/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/>Counterfactual explanations</a></h1>
<div>
A clear description counterfactual explanations, which is very important for human causal reasoning, is provided by Molnar & Dandl:
A counterfactual explanation describes a causal situation in the form: &ldquo;If X had not occurred, Y would not have occurred&rdquo;. For example: &ldquo;If I hadn&rsquo;t taken a sip of this hot coffee, I wouldn&rsquo;t have burned my tongue&rdquo;. Event Y is that I burned my tongue; cause X is that I had a hot coffee.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/example-based/>example-based</a></h1>
<div>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/global-surrogate/>Global surrogate models</a></h1>
<div>
One approach to explain predictions of &ldquo;black box&rdquo; models is to approximate its predictions with a white box model that is interpretable. A global surrogate model is a model that tries to do this approximation for a larger set of instances, ideally the whole input feature range. Also see local surrogate models.
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/gradient-based/>Gradient-based explanations</a></h1>
<div>
Gradient-based explanation methods use gradients (e.g. in deep neural networks) to evaluate the contribution of a model component on the model output. Some methods use gradients for evaluating the contribution of input features on the output, but others instead show the influence of a single (hidden) neuron on the output, or instead the influence of an input feature on a particular neuron&rsquo;s activation value.
In the case of computer vision (where deep convolutional neural networks are heavily used), one can show gradient-based information as a salience map.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/gradient-based/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ice-plot/>ICE plot</a></h1>
<div>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/local-surrogate/>Local surrogate models</a></h1>
<div>
One approach to explain predictions of &ldquo;black box&rdquo; models is to approximate its predictions with a white box model that is interpretable. A local surrogate model is a model that does this approximation more accurately only in the &ldquo;local&rdquo; feature space surrounding a single input. The idea is that even though a white box model may not accurately capture the behavior of a black box model globally (the feature space of a large set of training instances), it may e.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/local-surrogate/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/partial-dependence-plot/>partial dependence plot</a></h1>
<div>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/perturbation/>Perturbation</a></h1>
<div>
Explainability methods based on permutation explain the importance of a feature for a particular prediction by perturbating the feature and then investigating how this change affects the outcome.
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/sensitivity-analysis/>sensitivity analysis</a></h1>
<div>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley/>Shapley</a></h1>
<div>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley-value/>Shapley value explanations</a></h1>
<div>
Methods using Shapley values are very influential and use a particular form of perturbation to explain the importance of features for predictions. The importance of a feature for a prediction is measured in terms of how much this feature &ldquo;pushes&rdquo; the prediction away from a relevant baseline value, such as an average over a reference set. Different methods use different forms of perturbation. One could for example look at the difference in model output between 1) when a feature is present and 2) when a feature is occluded, repeat this for all subsets of features, and then compute the average difference per feature.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/shapley-value/>Read more...</a>
</div>
</article>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/white-box/>White box explanations</a></h1>
<div>
Typically a distinction is made between &ldquo;black box&rdquo; models and &ldquo;white box&rdquo; models. White box models are models where it is meaningful to look at the model&rsquo;s components. The most simple example is a linear regression problem, where the regression coefficients can (i.e. given proper preprocessing etc.) give insights into how much each feature contributes to the final prediction.
Some explainability packages do not try to explain &ldquo;black boxes&rdquo;, but instead propose using white box models either as a replacement for the black box model, or a surrogate to it.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/white-box/>Read more...</a>
</div>
</article>
</main>
<footer>
<p>Last modified on 29-07-2021.</p>
<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>
</body>
</html>