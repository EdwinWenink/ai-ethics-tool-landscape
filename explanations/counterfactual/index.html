<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Counterfactual explanations</title><link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css><link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/counterfactual/index.xml title="Ethical AI Tool Landscape"></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/values/>Values</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/stages/>Stages</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/taxonomy/>Taxonomy</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/>Tools</a></nav></header><main><div><h1 class=title>Counterfactual explanations</h1><div><p>A clear description counterfactual explanations, which is very important for human causal reasoning, is provided by Molnar & Dandl:</p><blockquote><p>A counterfactual explanation describes a causal situation in the form: &ldquo;If X had not occurred, Y would not have occurred&rdquo;. For example: &ldquo;If I hadn&rsquo;t taken a sip of this hot coffee, I wouldn&rsquo;t have burned my tongue&rdquo;. Event Y is that I burned my tongue; cause X is that I had a hot coffee. Thinking in counterfactuals requires imagining a hypothetical reality that contradicts the observed facts (for example, a world in which I have not drunk the hot coffee), hence the name &ldquo;counterfactual&rdquo;. The ability to think in counterfactuals makes us humans so smart compared to other animals. (<a href=https://christophm.github.io/interpretable-ml-book/counterfactual.html>Molnar & Dandl</a>)</p></blockquote><p>They also give a general approach for supporting counterfactual explanations about machine learning models:</p><blockquote><p>We simply change the feature values of an instance before making the predictions and we analyze how the prediction changes. We are interested in scenarios in which the prediction changes in a relevant way, like a flip in predicted class (for example, credit application accepted or rejected) or in which the prediction reaches a certain threshold (for example, the probability for cancer reaches 10%). A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output. (<a href=https://christophm.github.io/interpretable-ml-book/counterfactual.html>Molnar & Dandl</a>)</p></blockquote></div></div><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Alibi</a></h1><div>Alibi is an open-source Python library that supports various interpretability techniques and a broad array of explanation types. The README already provides an overview of the supported methods and when they are applicable. The following table with supported methods is copied from the README (slightly abbreviated):
Supported methods Method Models Explanations Classification Regression Tabular Text Images Categorical features ALE BB global ✔ ✔ ✔ Anchors BB local ✔ ✔ ✔ ✔ ✔ CEM BB* TF/Keras local ✔ ✔ ✔ Counterfactuals BB* TF/Keras local ✔ ✔ ✔ Prototype Counterfactuals BB* TF/Keras local ✔ ✔ ✔ ✔ Integrated Gradients TF/Keras local ✔ ✔ ✔ ✔ ✔ ✔ Kernel SHAP BB local global ✔ ✔ ✔ ✔ Tree SHAP WB local global ✔ ✔ ✔ ✔ The README also explains the keys:
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/dice/>DiCE: Diverse Counterfactual Explanations</a></h1><div>From README:
DiCE implements counterfactual (CF) explanations that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., you would have received the loan if your income was higher by $10,000. In other words, it provides &ldquo;what-if&rdquo; explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.
A main innovation of DiCE is that it implements a method to make producing counter-factual examples more model-agnostic:
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/dice/>Read more...</a></div></article></main><footer>Last modified on 30-06-2021.<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>