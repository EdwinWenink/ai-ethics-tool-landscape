<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<link rel="shortcut icon" href=img/favicon.ico>
<title>Individual Conditional Expectation (ICE)</title>
<link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css>
<link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ice-plot/index.xml title="Ethical AI Tool Landscape">
</head>
<body>
<header>
<div class=inline-block>
<a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a>
</div>
<nav>
<div id=breadcrumbs>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations>Explanations</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/ice-plot/>Ice plot</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a>
</div>
</nav>
</header>
<main>
<div>
<h1 class=title>Individual Conditional Expectation (ICE)</h1>
<div><p>An ICE plot can be seen as a
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/partial-dependence-plot/>partial dependence plot</a>
(PDP) that does not use the overal expectation of other features than the feature of interest, but instead the feature values of a single data point only.
So the reasoning behind an ICE plot is similar to a PDP, but the main difference is that whereas a PDP uses a &ldquo;global&rdquo; expectation over the other data points, the ICE plot contains a single line for each data instance.
Each line is computed by taking a data instance and varying only a feature of interest while keeping the other features the same, and then plotting the result on the predicted value for that instance.</p>
<p>An advantage of this approach over the PDP is that whereas a PDP is not accurate when there are (strong) correlations between features, an ICE plot will allow you to visualize and spot these correlations.</p>
<p>The following figure shows an ICE plot from Molnar&rsquo;s <a href=https://christophm.github.io/interpretable-ml-book/ice.html>Interpretable Machine Learning</a> book:</p>
<p><img src=https://christophm.github.io/interpretable-ml-book/images/ice-cervical-1.png alt="ICE plot example"></p>
</div>
</div>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/h2o-mli-resources/>H2O MLI Resources</a></h1>
<div>
This repository by H2O.ai contains useful resources and notebooks that showcase well-known machine learning interpretability techniques. The examples use the h2o Python package with their own estimators (e.g. their own fork of XGBoost), but all code is open-source and the examples are still illustrative of the interpretability techniques. These case studies that also deal with practical coding issues and preprocessing steps, e.g. that LIME can be unstable when there are strong correlations between input variables.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/h2o-mli-resources/>Read more...</a>
</div>
</article>
</main>
<footer>
<p>Last modified on 02-08-2021.</p>
<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>
</body>
</html>