<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<link rel="shortcut icon" href=img/favicon.ico>
<title>Anchor</title>
<link rel=stylesheet href=https://edwinwenink.github.io/ai-ethics-tool-landscape/css/style.css>
<link rel=alternate type=application/rss+xml href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/anchor/index.xml title="Ethical AI Tool Landscape">
</head>
<body>
<header>
<div class=inline-block>
<a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape target=_blank><img src=https://edwinwenink.github.io/ai-ethics-tool-landscape/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a>
</div>
<nav>
<div id=breadcrumbs>
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Home</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations>Explanations</a>
> <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/explanations/anchor/>Anchor</a>
| <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape//taxonomy>Taxonomy</a>
</div>
</nav>
</header>
<main>
<div>
<h1 class=title>Anchor</h1>
<div><p>An &ldquo;anchor&rdquo; is a subset of features and their value ranges for which the model will almost always output the same prediction.
Another way of saying this is that if the feature values of an anchor are satisfied, other features will very likely not affect the prediction, i.e. the prediction is &ldquo;anchored&rdquo;.
Anchors are by design interpretable, because they clearly indicate for which feature values they apply.
Such an anchor can be expressed as an <em>if-then</em> rule, but only in a specific feature range.
That is why Molnar calls them <a href=https://christophm.github.io/interpretable-ml-book/anchors.html>scoped rules</a>.
Algorithms for computing anchors are model-agnostic.</p>
</div>
</div>
<article>
<h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Alibi</a></h1>
<div>
Alibi is an open-source Python library that supports various interpretability techniques and a broad array of explanation types. The README already provides an overview of the supported methods and when they are applicable. The following table with supported methods is copied from the README (slightly abbreviated):
Supported methods Method Models Explanations Classification Regression Tabular Text Images Categorical features ALE BB global ✔ ✔ ✔ Anchors BB local ✔ ✔ ✔ ✔ ✔ CEM BB* TF/Keras local ✔ ✔ ✔ Counterfactuals BB* TF/Keras local ✔ ✔ ✔ Prototype Counterfactuals BB* TF/Keras local ✔ ✔ ✔ ✔ Integrated Gradients TF/Keras local ✔ ✔ ✔ ✔ ✔ ✔ Kernel SHAP BB local global ✔ ✔ ✔ ✔ Tree SHAP WB local global ✔ ✔ ✔ ✔ The README also explains the keys:
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/alibi/>Read more...</a>
</div>
</article>
</main>
<footer>
<p>Last modified on 02-08-2021.</p>
<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p>
</footer>
</body>
</html>