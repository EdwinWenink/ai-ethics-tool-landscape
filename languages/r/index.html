<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>R</title><link rel=stylesheet href=/ai-ethics-tool-landscape/css/style.css><link rel=alternate type=application/rss+xml href=/ai-ethics-tool-landscape/languages/r/index.xml title="Ethical AI Tool Landscape"></head><body><header><div class=inline-block><a href=https://github.com/EdwinWenink/ai-ethics-tool-landscape><img src=/img/GitHub-Mark/PNG/GitHub-Mark-32px.png alt="Github page" class=inline-logo></a></div><nav><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a>
> <a href=/ai-ethics-tool-landscape/ai-ethics-tool-landscape/values/>Values</a>
> <a href=/ai-ethics-tool-landscape/ai-ethics-tool-landscape/stages/>Stages</a>
> <a href=/ai-ethics-tool-landscape/ai-ethics-tool-landscape/categories/>Categories</a>
> <a href=/ai-ethics-tool-landscape/ai-ethics-tool-landscape/tools/>Tools</a>
> <a href=/ai-ethics-tool-landscape/ai-ethics-tool-landscape/taxonomy/>Taxonomy</a></nav></header><main><div><h1 class=title>R</h1></div><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ibm-ai-fairness-360/>IBM AI Fairness 360</a></h1><div>The IBM AI Fairness 360 Toolkit contains several bias mitigation algorithms that are applicable to various stages of the machine learning pipeline. The toolkit implements different notions of fairness, both on individual and the group level, and several fairness metrics for both classes of fairness. The toolkit provides additional guidance on choosing metrics and mitigation algorithms given a particular goal and application.
The following should be noted when using the fairness toolkit (and other similar toolkits, for that matter):
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/ibm-ai-fairness-360/>Read more...</a></div></article><article><h1><a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/lime/>LIME: Local Interpretable Model-agnostic Explanations</a></h1><div>The type of explanation LIME offers is a surrogate model that approximates a black box prediction locally. The surrogate model is a sparse linear model, which means that the surrogate model is interpretable (in this case, it&rsquo;s weights are meaningful). This simpler model can thus help to explain the black box prediction, assuming the local approximation is actually representative.
The intuition behind this is provided in the README:
Intuitively, an explanation is a local linear approximation of the model&rsquo;s behaviour.
<a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/tools/lime/>Read more...</a></div></article></main><footer>Last modified on 28-05-2021.<p>&copy; 2021 <a href=https://edwinwenink.github.io/ai-ethics-tool-landscape/>Ethical AI Tool Landscape</a> curated by <a href=https://www.edwinwenink.xyz>Edwin Wenink</a></p></footer></body></html>