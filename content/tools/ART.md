---
title: 'ART: Adversial Robustness 360 Toolbox'
values: ['safety']
categories: ['model-agnostic']
phases: ['model training']
licence: 'MIT'
repo: https://github.com/Trusted-AI/adversarial-robustness-toolbox 
languages: ['Python']
references: 
- 'Nicolae et al. - Adversarial Robustness Toolbox v1.0.0, https://arxiv.org/abs/1807.01069'
---

<!-- TODO check whether it's really model agnostic -->

From the Github repo:

> Adversarial Robustness Toolbox (ART) is a Python library for Machine Learning Security. ART provides tools that enable developers and researchers to defend and evaluate Machine Learning models and applications against the adversarial threats of Evasion, Poisoning, Extraction, and Inference. ART supports all popular machine learning frameworks (TensorFlow, Keras, PyTorch, MXNet, scikit-learn, XGBoost, LightGBM, CatBoost, GPy, etc.), all data types (images, tables, audio, video, etc.) and machine learning tasks (classification, object detection, speech recognition, generation, certification, etc.).
