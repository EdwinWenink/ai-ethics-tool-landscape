---
title: Perturbation
---

Explainability methods based on permutation explain the importance of a feature for a particular prediction by perturbating the feature and then investigating how this change affects the outcome, in particular whether the prediction error increases.
If perturbing a feature strongly increases prediction error, then this is an indication of this feature's importance.
This approach includes *permutation* feature importance.
